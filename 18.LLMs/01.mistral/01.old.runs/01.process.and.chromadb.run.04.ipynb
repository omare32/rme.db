{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pdfplumber\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from docx import Document\n",
    "from pptx import Presentation\n",
    "import warnings\n",
    "import re\n",
    "\n",
    "# Suppress specific CropBox warning from pdfplumber\n",
    "import logging\n",
    "logging.getLogger(\"pdfminer\").setLevel(logging.ERROR)\n",
    "\n",
    "class CropBoxFilter:\n",
    "    def filter(self, record):\n",
    "        return not (record.levelno == logging.WARNING and 'CropBox missing from /Page, defaulting to MediaBox' in record.getMessage())\n",
    "\n",
    "logging.getLogger().addFilter(CropBoxFilter())\n",
    "\n",
    "# Add tkinter for folder selection\n",
    "def select_folder():\n",
    "    import tkinter as tk\n",
    "    from tkinter import filedialog\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    folder_selected = filedialog.askdirectory(title='Select folder to process')\n",
    "    root.destroy()\n",
    "    return folder_selected\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text + \"\\n\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    return text.strip()\n",
    "\n",
    "def ocr_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                img = page.to_image(resolution=300).original\n",
    "                pil_img = Image.fromarray(img)\n",
    "                page_text = pytesseract.image_to_string(pil_img)\n",
    "                if page_text:\n",
    "                    text += page_text + \"\\n\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    return text.strip()\n",
    "\n",
    "def extract_text_from_excel(excel_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        xls = pd.ExcelFile(excel_path)\n",
    "        for sheet_name in xls.sheet_names:\n",
    "            df = pd.read_excel(xls, sheet_name=sheet_name, dtype=str)\n",
    "            text += f\"\\n--- Sheet: {sheet_name} ---\\n\"\n",
    "            text += df.fillna('').to_string(index=False, header=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return text.strip()\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        doc = Document(docx_path)\n",
    "        for para in doc.paragraphs:\n",
    "            text += para.text + \"\\n\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    return text.strip()\n",
    "\n",
    "def extract_text_from_pptx(pptx_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        prs = Presentation(pptx_path)\n",
    "        for slide in prs.slides:\n",
    "            for shape in slide.shapes:\n",
    "                if hasattr(shape, \"text\"):\n",
    "                    text += shape.text + \"\\n\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    return text.strip()\n",
    "\n",
    "def process_folder(folder_path, output_json_path):\n",
    "    data = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            print(f\"[INFO] Processing file: {file}\")\n",
    "            file_path = os.path.join(root, file)\n",
    "            ext = file.lower().split('.')[-1]\n",
    "            entry = {\n",
    "                'file_name': file,\n",
    "                'file_path': file_path,\n",
    "                'type': ext,\n",
    "                'extracted_at': datetime.now().isoformat(),\n",
    "                'text': ''\n",
    "            }\n",
    "            reason = None\n",
    "            if ext == 'pdf':\n",
    "                text = extract_text_from_pdf(file_path)\n",
    "                if not text:\n",
    "                    text = ocr_pdf(file_path)\n",
    "                    if text:\n",
    "                        reason = 'extracted via OCR'\n",
    "                    else:\n",
    "                        reason = 'no text extracted (PDF and OCR failed)'\n",
    "                else:\n",
    "                    reason = 'extracted as text PDF'\n",
    "                entry['text'] = text\n",
    "            elif ext in ['xls', 'xlsx']:\n",
    "                text = extract_text_from_excel(file_path)\n",
    "                entry['text'] = text\n",
    "                reason = 'extracted from Excel' if text else 'no text extracted (Excel)'\n",
    "            elif ext == 'docx':\n",
    "                text = extract_text_from_docx(file_path)\n",
    "                entry['text'] = text\n",
    "                reason = 'extracted from Word' if text else 'no text extracted (Word)'\n",
    "            elif ext == 'pptx':\n",
    "                text = extract_text_from_pptx(file_path)\n",
    "                entry['text'] = text\n",
    "                reason = 'extracted from PowerPoint' if text else 'no text extracted (PowerPoint)'\n",
    "            else:\n",
    "                print(f\"[WARN] Skipped unsupported file type: {file}\")\n",
    "                continue\n",
    "            if entry['text']:\n",
    "                data.append(entry)\n",
    "            else:\n",
    "                print(f\"[WARN] No text extracted from: {file} ({reason})\")\n",
    "    with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Processed {len(data)} documents. Output saved to {output_json_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # New output directory outside the repo\n",
    "    output_dir = r'C:/Users/Omar Essam2/OneDrive - Rowad Modern Engineering/x004 Data Science/03.rme.db/05.llm/extracted_json'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    folder = select_folder()\n",
    "    if not folder:\n",
    "        print(\"No folder selected. Exiting.\")\n",
    "    else:\n",
    "        folder_name = os.path.basename(os.path.normpath(folder))\n",
    "        output_json = os.path.join(output_dir, f\"{folder_name}_extracted.json\")\n",
    "        process_folder(folder, output_json) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load OpenAI API key from .env\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# New paths outside the repo\n",
    "EXTRACTED_DIR = r'C:/Users/Omar Essam2/OneDrive - Rowad Modern Engineering/x004 Data Science/03.rme.db/05.llm/extracted_json'\n",
    "CHROMA_DB_DIR = r'C:/Users/Omar Essam2/OneDrive - Rowad Modern Engineering/x004 Data Science/03.rme.db/05.llm/chroma_db'\n",
    "COLLECTION_NAME = 'company_docs'\n",
    "CHUNK_SIZE = 1000  # characters per chunk\n",
    "CHUNK_OVERLAP = 200\n",
    "ID_TRACK_FILE = os.path.join(CHROMA_DB_DIR, 'embedded_chunk_ids.txt')\n",
    "\n",
    "# Helper: chunk text\n",
    "def chunk_text(text, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "# Helper: get OpenAI embeddings\n",
    "def get_embedding(text):\n",
    "    resp = openai.embeddings.create(\n",
    "        input=[text],\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    return resp.data[0].embedding\n",
    "\n",
    "# Initialize Chroma DB\n",
    "client = chromadb.PersistentClient(path=CHROMA_DB_DIR, settings=Settings(allow_reset=True))\n",
    "if COLLECTION_NAME in [c.name for c in client.list_collections()]:\n",
    "    collection = client.get_collection(COLLECTION_NAME)\n",
    "else:\n",
    "    collection = client.create_collection(COLLECTION_NAME)\n",
    "\n",
    "def load_embedded_ids():\n",
    "    if not os.path.exists(ID_TRACK_FILE):\n",
    "        return set()\n",
    "    with open(ID_TRACK_FILE, 'r', encoding='utf-8') as f:\n",
    "        return set(line.strip() for line in f if line.strip())\n",
    "\n",
    "def save_embedded_id(chunk_id):\n",
    "    with open(ID_TRACK_FILE, 'a', encoding='utf-8') as f:\n",
    "        f.write(chunk_id + '\\n')\n",
    "\n",
    "def process_json_files():\n",
    "    files = [f for f in os.listdir(EXTRACTED_DIR) if f.endswith('.json')]\n",
    "    doc_count = 0\n",
    "    embedded_ids = load_embedded_ids()\n",
    "    for file in files:\n",
    "        with open(os.path.join(EXTRACTED_DIR, file), 'r', encoding='utf-8') as f:\n",
    "            docs = json.load(f)\n",
    "        for doc in tqdm(docs, desc=f\"Processing {file}\"):\n",
    "            text = doc['text']\n",
    "            if not text.strip():\n",
    "                continue\n",
    "            chunks = chunk_text(text)\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunk_id = f\"{doc['file_name']}_{i}\"\n",
    "                if chunk_id in embedded_ids:\n",
    "                    continue  # Skip already embedded chunk\n",
    "                meta = {\n",
    "                    'file_name': doc['file_name'],\n",
    "                    'file_path': doc['file_path'],\n",
    "                    'type': doc['type'],\n",
    "                    'extracted_at': doc['extracted_at'],\n",
    "                    'chunk': i\n",
    "                }\n",
    "                try:\n",
    "                    emb = get_embedding(chunk)\n",
    "                    collection.add(\n",
    "                        documents=[chunk],\n",
    "                        embeddings=[emb],\n",
    "                        metadatas=[meta],\n",
    "                        ids=[chunk_id]\n",
    "                    )\n",
    "                    save_embedded_id(chunk_id)\n",
    "                    doc_count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error embedding chunk: {e}\")\n",
    "    print(f\"Finished! {doc_count} new chunks embedded and stored in Chroma DB.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_json_files() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
