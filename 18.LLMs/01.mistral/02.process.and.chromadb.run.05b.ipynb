{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing file: F-P-TN-02-07 Sourcing Plan (003).xlsx | Project: 03 Procurement Follow Up Reports\n",
      "[WARN] No text extracted from: F-P-TN-02-07 Sourcing Plan (003).xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: Procurment Plans Status.xlsx | Project: 03 Procurement Follow Up Reports\n",
      "[WARN] No text extracted from: Procurment Plans Status.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 01- Egat Procuerment Plan 21-1-2023.xlsx | Project: 01 Mohamed El Saman Sector\n",
      "[WARN] No text extracted from: 01- Egat Procuerment Plan 21-1-2023.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 01- Eipico 3 Porcurement Plan 24.1.2023.xlsx | Project: 01 Mohamed El Saman Sector\n",
      "[WARN] No text extracted from: 01- Eipico 3 Porcurement Plan 24.1.2023.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 02- Eipico 3 Porcurement Plan 31-1-2023.xlsx | Project: 01 Mohamed El Saman Sector\n",
      "[WARN] No text extracted from: 02- Eipico 3 Porcurement Plan 31-1-2023.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 01- F-P-PC-01-05 Internal Sourcing Plan DPW 2- updated 31-1-2023.xlsx | Project: 01 Mohamed El Saman Sector\n",
      "[WARN] No text extracted from: 01- F-P-PC-01-05 Internal Sourcing Plan DPW 2- updated 31-1-2023.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 01- F-P-PC-01-06 Action Plan for NGUH Project - Updated 31-1-2023.xlsx | Project: 02 Osama Mezayn Sector\n",
      "[WARN] No text extracted from: 01- F-P-PC-01-06 Action Plan for NGUH Project - Updated 31-1-2023.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 01- Katmeya Creeks - Action Plan Update 24-01-2023.xlsx | Project: 02 Osama Mezayn Sector\n",
      "[WARN] No text extracted from: 01- Katmeya Creeks - Action Plan Update 24-01-2023.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 01- Mivida Procurment Plan 24-1-2023.xlsx | Project: 02 Osama Mezayn Sector\n",
      "[WARN] No text extracted from: 01- Mivida Procurment Plan 24-1-2023.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 01- Sourcing Plan - Update 31-1-2023.xlsx | Project: 05 Mohamed Abdelfath Sector\n",
      "[WARN] No text extracted from: 01- Sourcing Plan - Update 31-1-2023.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 01- Draft Procuerment Plan for Silos 5-2-2023.xlsx | Project: 01 Mohamed El Saman Sector\n",
      "[WARN] No text extracted from: 01- Draft Procuerment Plan for Silos 5-2-2023.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: Eipico 3 (Porcurement Plan) 14-2-2023 Arch.xlsx | Project: 03- Update 14-2-2023\n",
      "[WARN] No text extracted from: Eipico 3 (Porcurement Plan) 14-2-2023 Arch.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: Eipico 3 (Porcurement Plan) 14-2-2023 MEP.xlsx | Project: 03- Update 14-2-2023\n",
      "[WARN] No text extracted from: Eipico 3 (Porcurement Plan) 14-2-2023 MEP.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 01- Tarek abdelhakim Procurment Plan - 15-2-2023.xlsx | Project: 02 Feb-23\n",
      "[WARN] No text extracted from: 01- Tarek abdelhakim Procurment Plan - 15-2-2023.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 02- F-P-PC-01-06 Action Plan for NGUH Project -Updated 8-2-2023.xlsx | Project: 02 Osama Mezayn Sector\n",
      "[WARN] No text extracted from: 02- F-P-PC-01-06 Action Plan for NGUH Project -Updated 8-2-2023.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 03- F-P-PC-01-06 Action Plan for NGUH Project - Update 15-Feb-23.xlsx | Project: 02 Osama Mezayn Sector\n",
      "[WARN] No text extracted from: 03- F-P-PC-01-06 Action Plan for NGUH Project - Update 15-Feb-23.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 04- F-P-PC-01-06 Action Plan for NGUH Project - Update 22-Feb-23.xlsx | Project: 02 Osama Mezayn Sector\n",
      "[WARN] No text extracted from: 04- F-P-PC-01-06 Action Plan for NGUH Project - Update 22-Feb-23.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 02- F-P-PC-01-06 Action Plan - Update 07-Feb-2023.xlsx | Project: 02 Osama Mezayn Sector\n",
      "[WARN] No text extracted from: 02- F-P-PC-01-06 Action Plan - Update 07-Feb-2023.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: Action plan.xlsx | Project: 01- Update 6-2-2023\n",
      "[WARN] No text extracted from: Action plan.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: Dashboard.pdf | Project: 01- Update 6-2-2023\n",
      "[INFO] Processing file: Unifier.xlsx | Project: 01- Update 6-2-2023\n",
      "[WARN] No text extracted from: Unifier.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 02- Mivida Procurment Plan 7-2-2023.xlsx | Project: 02 Osama Mezayn Sector\n",
      "[WARN] No text extracted from: 02- Mivida Procurment Plan 7-2-2023.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 03- MIVIDA PKG #189 Unifier Plan 15-2-2023.xlsx | Project: 02 Osama Mezayn Sector\n",
      "[WARN] No text extracted from: 03- MIVIDA PKG #189 Unifier Plan 15-2-2023.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 04- MIVIDA PKG #189 Unifier Plan 22-2-2023.xlsx | Project: 02 Osama Mezayn Sector\n",
      "[WARN] No text extracted from: 04- MIVIDA PKG #189 Unifier Plan 22-2-2023.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: Air Defense - Procurement Plan - 16.02.2023.xlsx | Project: 03 Adel Rahmo Sector\n",
      "[WARN] No text extracted from: Air Defense - Procurement Plan - 16.02.2023.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 04- EIPICO 3 -  long Lead Items Tracking Log  - Arch - 19-Mar.xlsx | Project: 01 Mohamed El Saman Sector\n",
      "[WARN] No text extracted from: 04- EIPICO 3 -  long Lead Items Tracking Log  - Arch - 19-Mar.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 05- F-P-PC-01-06 Action Plan for NGUH Project - Update 01-Mar-23.xlsx | Project: 02 Osama Mezayn Sector\n",
      "[WARN] No text extracted from: 05- F-P-PC-01-06 Action Plan for NGUH Project - Update 01-Mar-23.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 06- F-P-PC-01-06 Action Plan for NGUH Project - Update 08-Mar-23.xlsx | Project: 02 Osama Mezayn Sector\n",
      "[WARN] No text extracted from: 06- F-P-PC-01-06 Action Plan for NGUH Project - Update 08-Mar-23.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 07- F-P-PC-01-06 Action Plan for NGUH Project - Update 16-Mar-23.xlsx | Project: 02 Osama Mezayn Sector\n",
      "[WARN] No text extracted from: 07- F-P-PC-01-06 Action Plan for NGUH Project - Update 16-Mar-23.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: Action plan.xlsx | Project: 02- Update 19-3-2023\n",
      "[WARN] No text extracted from: Action plan.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: Action plan[1].xlsx | Project: 02- Update 19-3-2023\n",
      "[WARN] No text extracted from: Action plan[1].xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: Sourcing plan Ph.02 - Update 19-03-2023.xlsx | Project: 02- Update 19-3-2023\n",
      "[WARN] No text extracted from: Sourcing plan Ph.02 - Update 19-03-2023.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 05- MIVIDA PKG #189 Unifier Plan 01-03-2023.xlsx | Project: 02 Osama Mezayn Sector\n",
      "[WARN] No text extracted from: 05- MIVIDA PKG #189 Unifier Plan 01-03-2023.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 06- MIVIDA PKG #189 Unifier Plan 15-03-2023.xlsx | Project: 02 Osama Mezayn Sector\n",
      "[WARN] No text extracted from: 06- MIVIDA PKG #189 Unifier Plan 15-03-2023.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 07- MIVIDA PKG #189 Unifier Plan 23-03-2023.xlsx | Project: 02 Osama Mezayn Sector\n",
      "[WARN] No text extracted from: 07- MIVIDA PKG #189 Unifier Plan 23-03-2023.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 05- EIPICO 3 -  long Lead Items Tracking Log  - Arch - 09-Apr.xlsx | Project: 01 Mohamed El Saman Sector\n",
      "[WARN] No text extracted from: 05- EIPICO 3 -  long Lead Items Tracking Log  - Arch - 09-Apr.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 08- MIVIDA PKG #189 Unifier Plan 06-04-2023.xlsx | Project: 02 Osama Mezayn Sector\n",
      "[WARN] No text extracted from: 08- MIVIDA PKG #189 Unifier Plan 06-04-2023.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: Dashboard.pdf | Project: 09-MIVIDA PKG #189 Unifier Plan 24-05-2023\n",
      "[INFO] Extracting archive: Fw_ Sourcing plan for Mivida PKG #189.zip\n",
      "[INFO] Processing file: Dashboard.pdf | Project: tmp9hexome2\n",
      "[INFO] Processing file: MIVIDA PKG #189Unifier Plan 24-05-2023.xlsx | Project: tmp9hexome2\n",
      "[WARN] No text extracted from: MIVIDA PKG #189Unifier Plan 24-05-2023.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: MIVIDA PKG #189Unifier Plan 24-05-2023.xlsx | Project: 09-MIVIDA PKG #189 Unifier Plan 24-05-2023\n",
      "[WARN] No text extracted from: MIVIDA PKG #189Unifier Plan 24-05-2023.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 08- F-P-PC-01-06 Action Plan for NGUH Project - Update 11-Jul-23.xlsx | Project: 02 Osama Mezayn Sector\n",
      "[WARN] No text extracted from: 08- F-P-PC-01-06 Action Plan for NGUH Project - Update 11-Jul-23.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 09 F-P-PC-01-06 Action Plan for NGUH Project - Update 25-Jul-23.pdf | Project: 02 Osama Mezayn Sector\n",
      "[INFO] Processing file: 03 F-P-PC-01-06 Action Plan - ARC,ELE,MEC Works - (22-07-2023).pdf | Project: 02 Osama Mezayn Sector\n",
      "[INFO] Processing file: 04 F-P-PC-01-06 Action Plan - ARC,ELE,MEC Works - (29-07-2023).pdf | Project: 02 Osama Mezayn Sector\n",
      "[INFO] Processing file: 10 F-P-PC-01-06 Action Plan for NGUH Project - Update 8-Aug-23.pdf | Project: 02 Osama Mezayn Sector\n",
      "[INFO] Processing file: 11 F-P-PC-01-06 Action Plan for NGUH Project - Update 15-Aug-23.pdf | Project: 02 Osama Mezayn Sector\n",
      "[INFO] Processing file: 12 F-P-PC-01-06 Action Plan for NGUH Project - Update 22-Aug-23.pdf | Project: 02 Osama Mezayn Sector\n",
      "[INFO] Processing file: 13 F-P-PC-01-06 Action Plan for NGUH Project - Update 29-Aug-23.pdf | Project: 02 Osama Mezayn Sector\n",
      "[INFO] Processing file: Action plan.xlsx | Project: 10-MIVIDA PKG #189 Unifier Plan 09-08-2023\n",
      "[WARN] No text extracted from: Action plan.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: Dashboard.pdf | Project: 10-MIVIDA PKG #189 Unifier Plan 09-08-2023\n",
      "[INFO] Processing file: Unifier.xlsx | Project: 10-MIVIDA PKG #189 Unifier Plan 09-08-2023\n",
      "[WARN] No text extracted from: Unifier.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 02 Admin Building-Action Plan-19-9-2023.xlsx | Project: 01 Mohamed El Saman Sector\n",
      "[WARN] No text extracted from: 02 Admin Building-Action Plan-19-9-2023.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 03 Admin Building-Internal Sourcing Plan 19-9-2023.xlsx | Project: 01 Mohamed El Saman Sector\n",
      "[WARN] No text extracted from: 03 Admin Building-Internal Sourcing Plan 19-9-2023.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 01 SPX - Action Plan 19-9-23.xlsx | Project: 01 Mohamed El Saman Sector\n",
      "[WARN] No text extracted from: 01 SPX - Action Plan 19-9-23.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 02 SPX - Remaining items Internal Sourcing Plan 19-9-23.xlsx | Project: 01 Mohamed El Saman Sector\n",
      "[WARN] No text extracted from: 02 SPX - Remaining items Internal Sourcing Plan 19-9-23.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 02 Silos Action Plan 19-9-23.xlsx | Project: 01 Mohamed El Saman Sector\n",
      "[WARN] No text extracted from: 02 Silos Action Plan 19-9-23.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 03 Silos Internal Sourcing Plan 19-9-23.xlsx | Project: 01 Mohamed El Saman Sector\n",
      "[WARN] No text extracted from: 03 Silos Internal Sourcing Plan 19-9-23.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 06 Internal Sourcing Plan - EPICO 3 19-9-23.xlsx | Project: 01 Mohamed El Saman Sector\n",
      "[WARN] No text extracted from: 06 Internal Sourcing Plan - EPICO 3 19-9-23.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 02 DPW02 - Internal Sourcing Plan - DD 19-9-23.xlsx | Project: 01 Mohamed El Saman Sector\n",
      "[WARN] No text extracted from: 02 DPW02 - Internal Sourcing Plan - DD 19-9-23.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 03 DPW02 - Action Plan 19-9-23.xlsx | Project: 01 Mohamed El Saman Sector\n",
      "[WARN] No text extracted from: 03 DPW02 - Action Plan 19-9-23.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 14 F-P-PC-01-06 Action Plan for NGUH Project - Update 5-Sep-23.pdf | Project: 02 Osama Mezayn Sector\n",
      "[INFO] Processing file: 15 F-P-PC-01-06 Action Plan for NGUH Project - Update 12-Sep-23.pdf | Project: 02 Osama Mezayn Sector\n",
      "[INFO] Processing file: 16 F-P-PC-01-06 Action Plan for NGUH Project - Update 19-Sep-23.pdf | Project: 02 Osama Mezayn Sector\n",
      "[INFO] Processing file: 17 F-P-PC-01-06 Action Plan for NGUH Project - Update 26-Sep-23.pdf | Project: 02 Osama Mezayn Sector\n",
      "[INFO] Processing file: 05 F-P-PC-01-06 Action Plan - ARC,ELE,MEC Works - (30-09-2023).pdf | Project: 02 Osama Mezayn Sector\n",
      "[INFO] Processing file: 04 Admin Building-Action Plan-15-10-2023.xlsx | Project: 01 Mohamed El Saman Sector\n",
      "[WARN] No text extracted from: 04 Admin Building-Action Plan-15-10-2023.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 05 Admin Building-Internal Sourcing Plan 15-10-2023.xlsx | Project: 01 Mohamed El Saman Sector\n",
      "[WARN] No text extracted from: 05 Admin Building-Internal Sourcing Plan 15-10-2023.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 04 Silos Action Plan 15-10-2023.xlsx | Project: 01 Mohamed El Saman Sector\n",
      "[WARN] No text extracted from: 04 Silos Action Plan 15-10-2023.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 05 Silos Internal Sourcing Plan 15-10-2023.xlsx | Project: 01 Mohamed El Saman Sector\n",
      "[WARN] No text extracted from: 05 Silos Internal Sourcing Plan 15-10-2023.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 18 F-P-PC-01-06 Action Plan for NGUH Project - Update 10-Oct-23.pdf | Project: 02 Osama Mezayn Sector\n",
      "[INFO] Processing file: 19 F-P-PC-01-06 Action Plan for NGUH Project - Update 17-Oct-23.pdf | Project: 02 Osama Mezayn Sector\n",
      "[INFO] Processing file: 20 F-P-PC-01-06 Action Plan for NGUH Project - Update 24-Oct-23.pdf | Project: 02 Osama Mezayn Sector\n",
      "[INFO] Processing file: 21 F-P-PC-01-06 Action Plan for NGUH Project - Update 31-Oct-23.pdf | Project: 02 Osama Mezayn Sector\n",
      "[INFO] Processing file: F-P-PC-01-06 Action Plan - ARC,ELE,MEC Works - (31-10-2023).pdf | Project: 02 Osama Mezayn Sector\n",
      "[INFO] Processing file: 11 MIVIDA PKG #189Unifier Plan 3-10-2023.xlsx | Project: 02 Osama Mezayn Sector\n",
      "[WARN] No text extracted from: 11 MIVIDA PKG #189Unifier Plan 3-10-2023.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: 12 MIVIDA PKG #189Unifier Plan.xlsx | Project: 02 Osama Mezayn Sector\n",
      "[WARN] No text extracted from: 12 MIVIDA PKG #189Unifier Plan.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: F-P-PC-01-06 Action Plan for NGUH Project - Update 7-Nov-23.pdf | Project: 02 Osama Mezayn Sector\n",
      "[INFO] Processing file: MIVIDA PKG #189Unifier Plan.xlsx | Project: 02 Osama Mezayn Sector\n",
      "[WARN] No text extracted from: MIVIDA PKG #189Unifier Plan.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: sourcing 2024 status.xlsx | Project: 03 Procurement Follow Up Reports\n",
      "[WARN] No text extracted from: sourcing 2024 status.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: mep 2024 sourcing plan.xlsx | Project: jan. 2024\n",
      "[WARN] No text extracted from: mep 2024 sourcing plan.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: Dr Hussein HSR Sohag Station Sourcing Plan Rev 00 @ 11-1-2024.xlsx | Project: plan\n",
      "[WARN] No text extracted from: Dr Hussein HSR Sohag Station Sourcing Plan Rev 00 @ 11-1-2024.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: F-P-TN-02-07 Sourcing Plan (003) Qasr Rashwan Projects.xlsx | Project: plans\n",
      "[WARN] No text extracted from: F-P-TN-02-07 Sourcing Plan (003) Qasr Rashwan Projects.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: F-P-TN-02-07 Sourcing Plan (003Abo shanb.xlsx | Project: plans\n",
      "[WARN] No text extracted from: F-P-TN-02-07 Sourcing Plan (003Abo shanb.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: Copy of F-P-TN-02-07 Sourcing Plan (003Abo shanb by heba.xlsx | Project: update\n",
      "[WARN] No text extracted from: Copy of F-P-TN-02-07 Sourcing Plan (003Abo shanb by heba.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: F-P-TN-02-07 CH  Qasr rashwan Sector Sourcing Plan - 04-Feb-24 -reviewed by heba.xlsx | Project: update\n",
      "[WARN] No text extracted from: F-P-TN-02-07 CH  Qasr rashwan Sector Sourcing Plan - 04-Feb-24 -reviewed by heba.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: F-P-TN-02-07 industrial Sourcing Plan (003) 1-2-2024.xlsx | Project: plan\n",
      "[WARN] No text extracted from: F-P-TN-02-07 industrial Sourcing Plan (003) 1-2-2024.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: Copy of Copy of ebico - reviewed by heba done 02.xlsx | Project: update\n",
      "[WARN] No text extracted from: Copy of Copy of ebico - reviewed by heba done 02.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: Copy of Copy of ebico - reviewed by heba done.xlsx | Project: update\n",
      "[WARN] No text extracted from: Copy of Copy of ebico - reviewed by heba done.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: Copy of F-P-TN-02-07 Sourcing Plan (003) mdf done.xlsx | Project: update\n",
      "[WARN] No text extracted from: Copy of F-P-TN-02-07 Sourcing Plan (003) mdf done.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: Copy of F-P-TN-02-07 Sourcing Plan suez  DPW SPX  silos done.xlsx | Project: update\n",
      "[WARN] No text extracted from: Copy of F-P-TN-02-07 Sourcing Plan suez  DPW SPX  silos done.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: F-P-TN-02-07 Sourcing Plan - C-R & H Sector 31.Dec.2023.xlsx | Project: plan\n",
      "[WARN] No text extracted from: F-P-TN-02-07 Sourcing Plan - C-R & H Sector 31.Dec.2023.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: Copy of F-P-TN-02-07 CH  R Sector Sourcing Plan - 04-Feb-24.xlsx | Project: update 4-2-2024\n",
      "[WARN] No text extracted from: Copy of F-P-TN-02-07 CH  R Sector Sourcing Plan - 04-Feb-24.xlsx (no text extracted (Excel))\n",
      "[INFO] Processing file: Copy of F-P-TN-02-07 CH  R Sector Sourcing Plan - 04-Feb-24 -reviewed by heba.xlsx | Project: verified by heba & departements\n",
      "[WARN] No text extracted from: Copy of F-P-TN-02-07 CH  R Sector Sourcing Plan - 04-Feb-24 -reviewed by heba.xlsx (no text extracted (Excel))\n",
      "Processed 22 documents. Output saved to C:/Users/Omar Essam2/OneDrive - Rowad Modern Engineering/x004 Data Science/03.rme.db/05.llm/extracted_json\\03 Procurement Follow Up Reports_extracted.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pdfplumber\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from docx import Document\n",
    "from pptx import Presentation\n",
    "import warnings\n",
    "import re\n",
    "import zipfile\n",
    "import rarfile\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "# Suppress specific CropBox warning from pdfplumber\n",
    "import logging\n",
    "logging.getLogger(\"pdfminer\").setLevel(logging.ERROR)\n",
    "\n",
    "class CropBoxFilter:\n",
    "    def filter(self, record):\n",
    "        return not (record.levelno == logging.WARNING and 'CropBox missing from /Page, defaulting to MediaBox' in record.getMessage())\n",
    "\n",
    "logging.getLogger().addFilter(CropBoxFilter())\n",
    "\n",
    "# Add tkinter for folder selection\n",
    "def select_folder():\n",
    "    import tkinter as tk\n",
    "    from tkinter import filedialog\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    folder_selected = filedialog.askdirectory(title='Select folder to process')\n",
    "    root.destroy()\n",
    "    return folder_selected\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text + \"\\n\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    return text.strip()\n",
    "\n",
    "def ocr_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                img = page.to_image(resolution=300).original\n",
    "                pil_img = Image.fromarray(img)\n",
    "                page_text = pytesseract.image_to_string(pil_img)\n",
    "                if page_text:\n",
    "                    text += page_text + \"\\n\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    return text.strip()\n",
    "\n",
    "def extract_project_name(file_path):\n",
    "    # Try to extract project name from folder path or filename\n",
    "    path_parts = os.path.normpath(file_path).split(os.sep)\n",
    "    # Remove common non-project folders\n",
    "    ignore = set(['', '2024', '2025', '2023', '2022', '2021', '2020', 'Mar. 2024', 'Jan-25', '3-Mar-2025', '01 Jan-25'])\n",
    "    # Look for a likely project name in the path\n",
    "    for part in reversed(path_parts[:-1]):\n",
    "        if part not in ignore and not re.match(r'\\d{4}', part):\n",
    "            return part\n",
    "    # Try to extract from filename\n",
    "    fname = os.path.basename(file_path)\n",
    "    match = re.search(r'([A-Za-z0-9\\- ]+Project|[A-Za-z0-9\\- ]+Dashboard|[A-Za-z0-9\\- ]+Lot|[A-Za-z0-9\\- ]+Port|[A-Za-z0-9\\- ]+Sector)', fname)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    # If nothing found, label as GLOBAL\n",
    "    return 'GLOBAL'\n",
    "\n",
    "def extract_text_from_excel(excel_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        ext = excel_path.lower().split('.')[-1]\n",
    "        if ext == 'xlsb':\n",
    "            xls = pd.ExcelFile(excel_path, engine='pyxlsb')\n",
    "        else:\n",
    "            xls = pd.ExcelFile(excel_path)\n",
    "        for sheet_name in xls.sheet_names:\n",
    "            df = pd.read_excel(xls, sheet_name=sheet_name, dtype=str)\n",
    "            # Only extract if <1000 rows and <30 columns and has headers\n",
    "            if df.shape[0] < 1000 and df.shape[1] < 30 and all(df.columns.str.strip() != ''):\n",
    "                text += f\"\\n--- Sheet: {sheet_name} ---\\n\"\n",
    "                text += df.fillna('').to_csv(index=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return text.strip()\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        doc = Document(docx_path)\n",
    "        for para in doc.paragraphs:\n",
    "            text += para.text + \"\\n\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    return text.strip()\n",
    "\n",
    "def extract_text_from_pptx(pptx_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        prs = Presentation(pptx_path)\n",
    "        for slide in prs.slides:\n",
    "            for shape in slide.shapes:\n",
    "                if hasattr(shape, \"text\"):\n",
    "                    text += shape.text + \"\\n\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    return text.strip()\n",
    "\n",
    "def extract_text_from_ppt(file_path):\n",
    "    \"\"\"Attempt to convert .ppt to .pptx using unoconv, then extract text.\"\"\"\n",
    "    import subprocess\n",
    "    import tempfile\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            pptx_path = os.path.join(tmpdir, os.path.basename(file_path) + '.pptx')\n",
    "            # Try to convert .ppt to .pptx using unoconv\n",
    "            result = subprocess.run([\n",
    "                'unoconv', '-f', 'pptx', '-o', pptx_path, file_path\n",
    "            ], capture_output=True)\n",
    "            if result.returncode == 0 and os.path.exists(pptx_path):\n",
    "                text = extract_text_from_pptx(pptx_path)\n",
    "            else:\n",
    "                print(f\"[WARN] Failed to convert .ppt to .pptx: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Exception during .ppt conversion: {file_path} | {e}\")\n",
    "    return text.strip()\n",
    "\n",
    "def extract_text_from_pbix(pbix_path):\n",
    "    \"\"\"Try to extract text from .pbix by unzipping and reading .json/.xml files inside.\"\"\"\n",
    "    import zipfile\n",
    "    import tempfile\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with zipfile.ZipFile(pbix_path, 'r') as z:\n",
    "            with tempfile.TemporaryDirectory() as tmpdir:\n",
    "                z.extractall(tmpdir)\n",
    "                for root, _, files in os.walk(tmpdir):\n",
    "                    for file in files:\n",
    "                        if file.endswith('.json') or file.endswith('.xml'):\n",
    "                            try:\n",
    "                                with open(os.path.join(root, file), 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                                    content = f.read()\n",
    "                                    # Only keep text if it's not too large\n",
    "                                    if len(content) < 500_000:\n",
    "                                        text += f\"\\n--- {file} ---\\n\" + content\n",
    "                            except Exception:\n",
    "                                continue\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to extract .pbix: {pbix_path} | {e}\")\n",
    "    return text.strip()\n",
    "\n",
    "def extract_text_from_doc(doc_path):\n",
    "    \"\"\"Convert .doc to .docx using unoconv, then extract text.\"\"\"\n",
    "    import subprocess\n",
    "    import tempfile\n",
    "    import os\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            docx_path = os.path.join(tmpdir, os.path.basename(doc_path) + '.docx')\n",
    "            result = subprocess.run([\n",
    "                'unoconv', '-f', 'docx', '-o', docx_path, doc_path\n",
    "            ], capture_output=True)\n",
    "            if result.returncode == 0 and os.path.exists(docx_path):\n",
    "                text = extract_text_from_docx(docx_path)\n",
    "            else:\n",
    "                print(f\"[WARN] Failed to convert .doc to .docx: {doc_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Exception during .doc conversion: {doc_path} | {e}\")\n",
    "    return text.strip()\n",
    "\n",
    "def process_file(file_path, data):\n",
    "    ext = file_path.lower().split('.')[-1]\n",
    "    project = extract_project_name(file_path)\n",
    "    print(f\"[INFO] Processing file: {os.path.basename(file_path)} | Project: {project}\")\n",
    "    entry = {\n",
    "        'file_name': os.path.basename(file_path),\n",
    "        'file_path': file_path,\n",
    "        'type': ext,\n",
    "        'project': project,\n",
    "        'extracted_at': datetime.now().isoformat(),\n",
    "        'text': ''\n",
    "    }\n",
    "    reason = None\n",
    "    if ext == 'pdf':\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "        if not text:\n",
    "            text = ocr_pdf(file_path)\n",
    "            if text:\n",
    "                reason = 'extracted via OCR'\n",
    "            else:\n",
    "                reason = 'no text extracted (PDF and OCR failed)'\n",
    "        else:\n",
    "            reason = 'extracted as text PDF'\n",
    "        entry['text'] = text\n",
    "    elif ext in ['xls', 'xlsx', 'xlsb']:\n",
    "        text = extract_text_from_excel(file_path)\n",
    "        entry['text'] = text\n",
    "        reason = 'extracted from Excel' if text else 'no text extracted (Excel)'\n",
    "    elif ext == 'docx':\n",
    "        text = extract_text_from_docx(file_path)\n",
    "        entry['text'] = text\n",
    "        reason = 'extracted from Word' if text else 'no text extracted (Word)'\n",
    "    elif ext in ['pptx', 'pptm']:\n",
    "        text = extract_text_from_pptx(file_path)\n",
    "        entry['text'] = text\n",
    "        reason = 'extracted from PowerPoint' if text else 'no text extracted (PowerPoint)'\n",
    "    elif ext == 'ppt':\n",
    "        text = extract_text_from_ppt(file_path)\n",
    "        entry['text'] = text\n",
    "        reason = 'extracted from PPT (via conversion)' if text else 'no text extracted (PPT)'\n",
    "    elif ext == 'pbix':\n",
    "        text = extract_text_from_pbix(file_path)\n",
    "        entry['text'] = text\n",
    "        reason = 'extracted from PBIX' if text else 'no text extracted (PBIX)'\n",
    "    elif ext == 'doc':\n",
    "        text = extract_text_from_doc(file_path)\n",
    "        entry['text'] = text\n",
    "        reason = 'extracted from DOC (via conversion)' if text else 'no text extracted (DOC)'\n",
    "    else:\n",
    "        print(f\"[WARN] Skipped unsupported file type: {os.path.basename(file_path)}\")\n",
    "        return\n",
    "    if entry['text']:\n",
    "        data.append(entry)\n",
    "    else:\n",
    "        print(f\"[WARN] No text extracted from: {os.path.basename(file_path)} ({reason})\")\n",
    "\n",
    "def process_folder(folder_path, output_json_path):\n",
    "    data = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            ext = file.lower().split('.')[-1]\n",
    "            if ext in ['zip', 'rar']:\n",
    "                print(f\"[INFO] Extracting archive: {file}\")\n",
    "                with tempfile.TemporaryDirectory() as tmpdir:\n",
    "                    try:\n",
    "                        if ext == 'zip':\n",
    "                            with zipfile.ZipFile(file_path, 'r') as z:\n",
    "                                z.extractall(tmpdir)\n",
    "                        elif ext == 'rar':\n",
    "                            with rarfile.RarFile(file_path, 'r') as r:\n",
    "                                r.extractall(tmpdir)\n",
    "                        # Recursively process extracted files\n",
    "                        for subroot, _, subfiles in os.walk(tmpdir):\n",
    "                            for subfile in subfiles:\n",
    "                                subfile_path = os.path.join(subroot, subfile)\n",
    "                                process_file(subfile_path, data)\n",
    "                    except Exception as e:\n",
    "                        print(f\"[WARN] Failed to extract {file}: {e}\")\n",
    "            else:\n",
    "                process_file(file_path, data)\n",
    "    with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Processed {len(data)} documents. Output saved to {output_json_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # New output directory outside the repo\n",
    "    output_dir = r'C:/Users/Omar Essam2/OneDrive - Rowad Modern Engineering/x004 Data Science/03.rme.db/05.llm/extracted_json'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    folder = select_folder()\n",
    "    if not folder:\n",
    "        print(\"No folder selected. Exiting.\")\n",
    "    else:\n",
    "        folder_name = os.path.basename(os.path.normpath(folder))\n",
    "        output_json = os.path.join(output_dir, f\"{folder_name}_extracted.json\")\n",
    "        process_folder(folder, output_json) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 01 Dash Boards_extracted.json: 100%|██████████| 256/256 [00:00<00:00, 624.78it/s]\n",
      "Processing 02 Monthly Progress Presentation_extracted.json: 100%|██████████| 574/574 [00:00<00:00, 26933.19it/s]\n",
      "Processing 03 Procurement Follow Up Reports_extracted.json: 100%|██████████| 22/22 [00:04<00:00,  5.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished! 303 new chunks embedded and stored in Chroma DB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Requirements: pip install sentence-transformers torch chromadb tqdm python-dotenv\n",
    "import os\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load environment variables (if needed for paths)\n",
    "load_dotenv()\n",
    "\n",
    "# New paths outside the repo\n",
    "EXTRACTED_DIR = r'C:/Users/Omar Essam2/OneDrive - Rowad Modern Engineering/x004 Data Science/03.rme.db/05.llm/extracted_json'\n",
    "CHROMA_DB_DIR = r'C:/Users/Omar Essam2/OneDrive - Rowad Modern Engineering/x004 Data Science/03.rme.db/05.llm/chroma_db_local'\n",
    "COLLECTION_NAME = 'company_docs_local'\n",
    "CHUNK_SIZE = 500  # characters per chunk (reduced for more precise retrieval)\n",
    "CHUNK_OVERLAP = 200\n",
    "ID_TRACK_FILE = os.path.join(CHROMA_DB_DIR, 'embedded_chunk_ids.txt')\n",
    "BATCH_SIZE = 32  # Number of chunks to embed in one call\n",
    "\n",
    "# Helper: chunk text\n",
    "def chunk_text(text, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "# Helper: get local embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # You can change to another local model if needed\n",
    "def get_embeddings(texts):\n",
    "    return model.encode(texts, show_progress_bar=False, convert_to_numpy=True).tolist()\n",
    "\n",
    "# Initialize Chroma DB\n",
    "client = chromadb.PersistentClient(path=CHROMA_DB_DIR, settings=Settings(allow_reset=True))\n",
    "if COLLECTION_NAME in [c.name for c in client.list_collections()]:\n",
    "    collection = client.get_collection(COLLECTION_NAME)\n",
    "else:\n",
    "    collection = client.create_collection(COLLECTION_NAME)\n",
    "\n",
    "def load_embedded_ids():\n",
    "    if not os.path.exists(ID_TRACK_FILE):\n",
    "        return set()\n",
    "    with open(ID_TRACK_FILE, 'r', encoding='utf-8') as f:\n",
    "        return set(line.strip() for line in f if line.strip())\n",
    "\n",
    "def save_embedded_id(chunk_id):\n",
    "    with open(ID_TRACK_FILE, 'a', encoding='utf-8') as f:\n",
    "        f.write(chunk_id + '\\n')\n",
    "\n",
    "def process_json_files():\n",
    "    files = [f for f in os.listdir(EXTRACTED_DIR) if f.endswith('.json')]\n",
    "    doc_count = 0\n",
    "    embedded_ids = load_embedded_ids()\n",
    "    for file in files:\n",
    "        with open(os.path.join(EXTRACTED_DIR, file), 'r', encoding='utf-8') as f:\n",
    "            docs = json.load(f)\n",
    "        for doc in tqdm(docs, desc=f\"Processing {file}\"):\n",
    "            text = doc['text']\n",
    "            if not text.strip():\n",
    "                continue\n",
    "            chunks = chunk_text(text)\n",
    "            batch_chunks = []\n",
    "            batch_metas = []\n",
    "            batch_ids = []\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunk_id = f\"{doc['file_name']}_{i}\"\n",
    "                if chunk_id in embedded_ids:\n",
    "                    continue  # Skip already embedded chunk\n",
    "                meta = {\n",
    "                    'file_name': doc['file_name'],\n",
    "                    'file_path': doc['file_path'],\n",
    "                    'type': doc['type'],\n",
    "                    'project': doc.get('project', 'GLOBAL'),\n",
    "                    'extracted_at': doc['extracted_at'],\n",
    "                    'chunk': i\n",
    "                }\n",
    "                batch_chunks.append(chunk)\n",
    "                batch_metas.append(meta)\n",
    "                batch_ids.append(chunk_id)\n",
    "                if len(batch_chunks) == BATCH_SIZE:\n",
    "                    try:\n",
    "                        embs = get_embeddings(batch_chunks)\n",
    "                        collection.add(\n",
    "                            documents=batch_chunks,\n",
    "                            embeddings=embs,\n",
    "                            metadatas=batch_metas,\n",
    "                            ids=batch_ids\n",
    "                        )\n",
    "                        for cid in batch_ids:\n",
    "                            save_embedded_id(cid)\n",
    "                        doc_count += len(batch_chunks)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error embedding batch: {e}\")\n",
    "                    batch_chunks, batch_metas, batch_ids = [], [], []\n",
    "            # Process any remaining chunks in the batch\n",
    "            if batch_chunks:\n",
    "                try:\n",
    "                    embs = get_embeddings(batch_chunks)\n",
    "                    collection.add(\n",
    "                        documents=batch_chunks,\n",
    "                        embeddings=embs,\n",
    "                        metadatas=batch_metas,\n",
    "                        ids=batch_ids\n",
    "                    )\n",
    "                    for cid in batch_ids:\n",
    "                        save_embedded_id(cid)\n",
    "                    doc_count += len(batch_chunks)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error embedding batch: {e}\")\n",
    "    print(f\"Finished! {doc_count} new chunks embedded and stored in Chroma DB.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_json_files() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
