{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa30371c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Page 1 ---\n",
      "Hazem Omar Mohamed Atwa\n",
      "\n",
      "Cairo, Egypt. & 01118559305\n",
      "& hazematwwa@gmai.com\n",
      "fia linkedin.com/in/hazem-omar\n",
      "\n",
      "My objective is to get involved in various large-scale projects that are related to data engineering including\n",
      "creating data pipelines, ETL/ELT, data-oriented system design, etc. Also, to be a team player and to interact\n",
      "with different mindsets and cultures to push myself and my company forward.\n",
      "\n",
      "EXPERIENCE\n",
      "\n",
      "Sr. Data Engineer & Cloud | Dotpy (FULL Time)\n",
      "JUN 2023 — PRESEN\n",
      "\n",
      "As part of Al-Pro track at Information technology institute (ITI), | conduct cloud and big data\n",
      "fundamental’s courses and implement them on AWS.\n",
      "\n",
      "Big Data Topics:\n",
      "\n",
      "e Basic storage and database systems (OLAP and OLTP, Row vs columnar storage, ACID\n",
      "transactions, CAP theorem, etc.)\n",
      "\n",
      "e Hadoop ecosystem (HDFS, MapReduce, Hive, etc)\n",
      "\n",
      "e Apache Hive and Apache Spark Architecture and use cases.\n",
      "\n",
      "e NoSql-db like mongo-db and cassendra to how handle schema less with a lot of\n",
      "changeable of data\n",
      "\n",
      "Cloud Topics:\n",
      "\n",
      "e Mainly AWS, explaining basic AWS services like Networking, IAM, $3, EC2, etc.\n",
      "e MS Azure to build a pipeline from end-to-end\n",
      "\n",
      "Teaching Assist (TA) | AAST | CS Faculty (SESSIONAL)\n",
      "APR 2022 — 2024 OCT\n",
      "\n",
      "Teaching Assistant in Computer and Artificial Intelligence Faculty) .Specialist in Al & Big Data\n",
      "Department\n",
      "\n",
      "Courses Related for Big Data:\n",
      "\n",
      "e Database Theory (OLTP)\n",
      "\n",
      "e DWH (OLAP)\n",
      "\n",
      "e Data Mining\n",
      "\n",
      "e Big Data & Streaming Analysis (Hadoop Echo System)\n",
      "e Python Programming Language\n",
      "\n",
      "e Linux Administration (Bash Script)\n",
      "\n",
      "Data engineer II | Emuratic solution (FULL Time)\n",
      "MAY 2021 — MAR 2023\n",
      "\n",
      "e Use Talend for big data to design, develop and automate data pipelines that perform\n",
      "a specific task -usually data transformation and integration-\n",
      "\n",
      "--- Page 2 ---\n",
      "e Develop Python Py/SQL code and sometimes integrate that code in Talend jobs to\n",
      "\n",
      "automate it.\n",
      "\n",
      "e Write Linux bash scripts and use SQL*Loader on Linux to load very large files to the\n",
      "database.\n",
      "\n",
      "e Deal with vendors from different companies and facilitate/maintain their\n",
      "applications.\n",
      "\n",
      "1- Create distributed system for how treat with big data:\n",
      "a. Create distributed system with different machine and make a network\n",
      "between all this machine and store the\n",
      "2- Create a pipeline to handle a different velocity:\n",
      "Use all tools to how make a pipeline for how treat with verity of data and process it,\n",
      "using a batch query processing and real time analysis with different services.\n",
      "3- DevOps and CRC:\n",
      "Achieved required DevOps tasks that ease team daily operations:\n",
      "a. Implemented a CI/CD environment using Gitlab.\n",
      "b. Installed and configured services with containerization technologies using\n",
      "Docker\n",
      "\n",
      "ETL Developer | Vezzeta (FULL Time)\n",
      "MAY 2018 — MAR 2020\n",
      "\n",
      "In this role, | help clients design, deploy, maintain, modernize and migrate data lakes, data\n",
      "warehouses and data pipelines using cloud technologies, mainly using AWS. Projects:\n",
      "\n",
      "1. Major data lake migration and modernization:\n",
      "My contributions involved redesigning data pipelines using AWS services,\n",
      "refactoring legacy codes, configuring Cl/CD pipeline and monitoring dashboards.\n",
      "Technologies used:\n",
      "e Scala Spark on AWS EMR for processing.\n",
      "e  Elasticsearch for logging and monitoring.\n",
      "e Apache Airflow for orchestration.\n",
      "e Write laC using CDK to provision all resources.\n",
      "2. Migrate and Redesign SQL server Architecture:\n",
      "That involved designing a highly available and disaster recoverable SQL Server\n",
      "instance installed on EC2 in a multi-AZ architecture. Technologies used: SQL Server on\n",
      "EC2 including Always on availability group and log shipping between 2 availability\n",
      "zones.\n",
      "\n",
      "EDUCATION\n",
      "\n",
      "BSC of Computer Science | Faculty of CS & Al | MTI\n",
      "OCT 2014 — JUN 2018\n",
      "\n",
      "Major: Computer Science, GPA: 3.4\n",
      "Minor: Computer Science (CS)\n",
      "\n",
      "Graduation Project: Build a full software cycle of development in health and blood donation\n",
      "system to extract the main knowledge help CEO to make a decision on it\n",
      "\n",
      "Grade: A\n",
      "Certification:\n",
      "\n",
      "e AWS Certified Data Analytics — Specialty, 2023.\n",
      "e Google Cloud Professional Data Engineer, 2023\n",
      "e AWS Certified Solutions Architect — Associate, 2021.\n",
      "\n",
      "--- Page 3 ---\n",
      "MSC of Computer Science | Faculty of CS | AAST\n",
      "JAN 2021 — APR 2024\n",
      "\n",
      "Major: Computer Science, GPA:3.8\n",
      "Minor: Computer Science (CS)\n",
      "\n",
      "Research Topic: Especially Big Data Field in Hadoop echo system in job scheduling and how\n",
      "make a fair scheduling with very large amount of data and high traffic jobs without missing jobs\n",
      "\n",
      "Related coursework:\n",
      "\n",
      "e Cloud DevOps Engineer nanodegree — Udacity, 2020.\n",
      "\n",
      "e Big Data & Data Lake Specialization — Coursera, 2021.\n",
      "\n",
      "e Python and MySQL courses — Udemy, 2020.\n",
      "\n",
      "e RDBMS for (OLTP) - IBM 2020.\n",
      "\n",
      "e DWH for (OLAP) — Coursera 2021.\n",
      "\n",
      "e —LPIC-1 Linux system administration — LPI Egypt partner, 2022.\n",
      "e Docker & Kubernetes for data engineer\n",
      "\n",
      "SKILLS AND ACTIVITES\n",
      "\n",
      "e Cloud skills specially AWS.\n",
      "e Linux scripting and administration.\n",
      "e Git and version control.\n",
      "e Docker and Kubernetes on AWS.\n",
      "e DevOps with CircleCl, and Jenkins with Blue Ocean.\n",
      "e 3D design with Solidworks.\n",
      "e Languages | speak:\n",
      "1- Arabic: Native.\n",
      "2- English: Fluent.\n",
      "3- French: B1.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import os\n",
    "from tkinter import Tk\n",
    "from tkinter.filedialog import askopenfilename\n",
    "\n",
    "# Optional: specify tesseract path if not in PATH\n",
    "# pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    pages = convert_from_path(pdf_path)\n",
    "\n",
    "    all_text = \"\"\n",
    "    for i, page in enumerate(pages):\n",
    "        image_path = f\"page_{i}.png\"\n",
    "        page.save(image_path, \"PNG\")\n",
    "\n",
    "        text = pytesseract.image_to_string(Image.open(image_path))\n",
    "        all_text += f\"\\n--- Page {i + 1} ---\\n{text}\"\n",
    "\n",
    "        os.remove(image_path)\n",
    "\n",
    "    return all_text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Launch file picker dialog\n",
    "    Tk().withdraw()  # Hide the root window\n",
    "    pdf_file = askopenfilename(filetypes=[(\"PDF files\", \"*.pdf\")])\n",
    "\n",
    "    if pdf_file:\n",
    "        extracted_text = extract_text_from_pdf(pdf_file)\n",
    "        print(extracted_text)\n",
    "    else:\n",
    "        print(\"No file selected.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
