{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to 10.10.11.242...\n",
      "Executing: cd ~/airflow\n",
      "Executing: source airflow/venv/bin/activate && airflow db migrate\n",
      "Output: DB: mysql+mysqldb://gamal:***@10.10.11.242/RME_DH\n",
      "Performing upgrade to the metadata database mysql+mysqldb://gamal:***@10.10.11.242/RME_DH\n",
      "[2025-04-29T18:03:34.675+0300] {migration.py:207} INFO - Context impl MySQLImpl.\n",
      "[2025-04-29T18:03:34.675+0300] {migration.py:210} INFO - Will assume non-transactional DDL.\n",
      "[2025-04-29T18:03:34.678+0300] {migration.py:207} INFO - Context impl MySQLImpl.\n",
      "[2025-04-29T18:03:34.679+0300] {migration.py:210} INFO - Will assume non-transactional DDL.\n",
      "[2025-04-29T18:03:34.680+0300] {db.py:1675} INFO - Creating tables\n",
      "Database migrating done!\n",
      "\n",
      "Error: INFO  [alembic.runtime.migration] Context impl MySQLImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "INFO  [alembic.runtime.migration] Context impl MySQLImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "\n",
      "Executing: source airflow/venv/bin/activate && airflow scheduler -D\n",
      "Output:   ____________       _____________\n",
      " ____    |__( )_________  __/__  /________      __\n",
      "____  /| |_  /__  ___/_  /_ __  /_  __ \\_ | /| / /\n",
      "___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /\n",
      " _/_/  |_/_/  /_/    /_/    /_/  \\____/____/|__/\n",
      "[2025-04-29T18:03:37.092+0300] {_client.py:1025} INFO - HTTP Request: GET https://apacheairflow.gateway.scarf.sh/scheduler?version=2.10.4&python_version=3.12&platform=Linux&arch=x86_64&database=mysql&db_version=8.0&executor=LocalExecutor \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Error: Traceback (most recent call last):\n",
      "  File \"/home/PMO/airflow/venv/bin/airflow\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/home/PMO/airflow/venv/lib/python3.12/site-packages/airflow/__main__.py\", line 62, in main\n",
      "    args.func(args)\n",
      "  File \"/home/PMO/airflow/venv/lib/python3.12/site-packages/airflow/cli/cli_config.py\", line 49, in command\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/PMO/airflow/venv/lib/python3.12/site-packages/airflow/utils/cli.py\", line 116, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/PMO/airflow/venv/lib/python3.12/site-packages/airflow/utils/providers_configuration_loader.py\", line 55, in wrapped_function\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/PMO/airflow/venv/lib/python3.12/site-packages/airflow/cli/commands/scheduler_command.py\", line 59, in scheduler\n",
      "    run_command_with_daemon_option(\n",
      "  File \"/home/PMO/airflow/venv/lib/python3.12/site-packages/airflow/cli/commands/daemon_utils.py\", line 58, in run_command_with_daemon_option\n",
      "    check_if_pidfile_process_is_running(pid_file=pid, process_name=process_name)\n",
      "  File \"/home/PMO/airflow/venv/lib/python3.12/site-packages/airflow/utils/process_utils.py\", line 328, in check_if_pidfile_process_is_running\n",
      "    raise AirflowException(f\"The {process_name} is already running under PID {pid}.\")\n",
      "airflow.exceptions.AirflowException: The scheduler is already running under PID 15777.\n",
      "\n",
      "Executing: source airflow/venv/bin/activate && airflow webserver -D --port 8090\n",
      "Output:   ____________       _____________\n",
      " ____    |__( )_________  __/__  /________      __\n",
      "____  /| |_  /__  ___/_  /_ __  /_  __ \\_ | /| / /\n",
      "___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /\n",
      " _/_/  |_/_/  /_/    /_/    /_/  \\____/____/|__/\n",
      "Running the Gunicorn Server with:\n",
      "Workers: 4 sync\n",
      "Host: 0.0.0.0:8090\n",
      "Timeout: 120\n",
      "Logfiles: - -\n",
      "Access Logformat: \n",
      "=================================================================\n",
      "[2025-04-29T18:03:38.826+0300] {dagbag.py:588} INFO - Filling up the DagBag from /dev/null\n",
      "\n",
      "Error: /home/PMO/airflow/venv/lib/python3.12/site-packages/flask_limiter/extension.py:333 UserWarning: Using the in-memory storage for tracking rate limits as no storage was explicitly specified. This is not recommended for production use. See: https://flask-limiter.readthedocs.io#configuring-a-storage-backend for documentation about configuring the storage backend.\n",
      "/home/PMO/airflow/venv/lib/python3.12/site-packages/airflow/api_connexion/schemas/task_schema.py:52 ChangedInMarshmallow4Warning: `Number` field should not be instantiated. Use `Integer`, `Float`, or `Decimal` instead.\n",
      "/home/PMO/airflow/venv/lib/python3.12/site-packages/airflow/api_connexion/schemas/task_schema.py:55 ChangedInMarshmallow4Warning: `Number` field should not be instantiated. Use `Integer`, `Float`, or `Decimal` instead.\n",
      "/home/PMO/airflow/venv/lib/python3.12/site-packages/airflow/api_connexion/schemas/task_schema.py:59 ChangedInMarshmallow4Warning: `Number` field should not be instantiated. Use `Integer`, `Float`, or `Decimal` instead.\n",
      "/home/PMO/airflow/venv/lib/python3.12/site-packages/airflow/www/app.py:178 RemovedInAirflow3Warning: The experimental REST API is deprecated. Please migrate to the stable REST API. Please note that the experimental API do not have access control. The authenticated user has full access.\n",
      "\n",
      "Executing: echo PMO@1234 | sudo -S /opt/spark/sbin/start-master.sh -i 10.10.11.242 --webui-port 8080\n",
      "Output: starting org.apache.spark.deploy.master.Master, logging to /opt/spark/logs/spark-root-org.apache.spark.deploy.master.Master-1-PMO.out\n",
      "\n",
      "Error: [sudo] password for PMO: \n",
      "Executing: echo PMO@1234 | sudo -S /opt/spark/sbin/start-worker.sh spark://10.10.11.242:7077\n",
      "Output: starting org.apache.spark.deploy.worker.Worker, logging to /opt/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-PMO.out\n",
      "\n",
      "Error: [sudo] password for PMO: \n",
      "Executing: echo PMO@1234 | sudo -S jps\n",
      "Output: 72825 Jps\n",
      "72571 Master\n",
      "72703 Worker\n",
      "\n",
      "Error: [sudo] password for PMO: \n",
      "All commands executed successfully.\n"
     ]
    }
   ],
   "source": [
    "import paramiko\n",
    "\n",
    "# SSH details\n",
    "host = \"10.10.11.242\"\n",
    "username = \"PMO\"\n",
    "password = \"PMO@1234\"\n",
    "sudo_password = \"PMO@1234\"\n",
    "\n",
    "# Commands to run on the server\n",
    "commands = [\n",
    "    \"cd ~/airflow\",\n",
    "    \"source airflow/venv/bin/activate && airflow db migrate\",\n",
    "    \"source airflow/venv/bin/activate && airflow scheduler -D\",\n",
    "    \"source airflow/venv/bin/activate && airflow webserver -D --port 8090\",\n",
    "    f\"echo {sudo_password} | sudo -S /opt/spark/sbin/start-master.sh -i 10.10.11.242 --webui-port 8080\",\n",
    "    f\"echo {sudo_password} | sudo -S /opt/spark/sbin/start-worker.sh spark://10.10.11.242:7077\",\n",
    "    f\"echo {sudo_password} | sudo -S jps\"\n",
    "]\n",
    "\n",
    "def execute_ssh_commands(host, username, password, commands):\n",
    "    try:\n",
    "        # Create SSH client\n",
    "        client = paramiko.SSHClient()\n",
    "        client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "\n",
    "        # Connect to the server\n",
    "        print(f\"Connecting to {host}...\")\n",
    "        client.connect(hostname=host, username=username, password=password)\n",
    "\n",
    "        # Execute each command\n",
    "        for command in commands:\n",
    "            print(f\"Executing: {command}\")\n",
    "            stdin, stdout, stderr = client.exec_command(command)\n",
    "            stdout_result = stdout.read().decode()\n",
    "            stderr_result = stderr.read().decode()\n",
    "\n",
    "            if stdout_result:\n",
    "                print(f\"Output: {stdout_result}\")\n",
    "            if stderr_result:\n",
    "                print(f\"Error: {stderr_result}\")\n",
    "\n",
    "        # Close the connection\n",
    "        client.close()\n",
    "        print(\"All commands executed successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Run the function\n",
    "execute_ssh_commands(host, username, password, commands)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
