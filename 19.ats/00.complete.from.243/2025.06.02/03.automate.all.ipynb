{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CRM] Starting CRM fetch and download process...\n",
      "[CRM] Fetching and downloading from CRM complete.\n",
      "\n",
      "[OCR] Starting OCR extraction for new PDFs...\n",
      "[OCR] OCR extraction complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Download CVs from CRM and update database\n",
    "print(\"[CRM] Starting CRM fetch and download process...\")\n",
    "def get_session():\n",
    "    session = requests.Session()\n",
    "    session.auth = HttpNtlmAuth(USERNAME, PASSWORD)\n",
    "    session.headers.update({\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"OData-MaxVersion\": \"4.0\",\n",
    "        \"OData-Version\": \"4.0\"\n",
    "    })\n",
    "    return session\n",
    "\n",
    "def get_last_extracted_date():\n",
    "    conn = psycopg2.connect(**DB_CONFIG)\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        cursor.execute(\"SELECT MAX(createdon) FROM pdf_extracted_data\")\n",
    "        last_date = cursor.fetchone()[0]\n",
    "        if last_date:\n",
    "            print(f\"[CRM] Last extracted date from DB: {last_date}\")\n",
    "            return last_date\n",
    "        else:\n",
    "            print(\"[CRM] No previous extraction found, defaulting to 3 days ago.\")\n",
    "            return datetime.now() - timedelta(days=3)\n",
    "    except Exception as e:\n",
    "        print(f\"[CRM][ERROR] Error fetching last extracted date: {e}\")\n",
    "        return datetime.now() - timedelta(days=3)\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "def get_job_applications(session):\n",
    "    all_applications = []\n",
    "    start_date = get_last_extracted_date()\n",
    "    end_date = datetime.now()\n",
    "    print(f\"[CRM] Fetching applications from {start_date} to {end_date.date()}\")\n",
    "    filter_condition = (\n",
    "        f\"createdon ge {start_date.strftime('%Y-%m-%dT00:00:00Z')} \"\n",
    "        f\"and createdon le {end_date.strftime('%Y-%m-%dT23:59:59Z')}\"\n",
    "    )\n",
    "    url = (\n",
    "        f\"{CRM_URL}/new_jobapplications?\"\n",
    "        f\"$select=new_jobapplicationid,new_name,new_email,new_jauid,createdon&\"\n",
    "        f\"$filter={quote(filter_condition)}&\"\n",
    "        \"$top=5000\"\n",
    "    )\n",
    "    try:\n",
    "        response = session.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        if \"value\" in data:\n",
    "            applications = data[\"value\"]\n",
    "            print(f\"[CRM] Found {len(applications)} applications since last extraction\")\n",
    "            for app in applications:\n",
    "                annotations_url = (\n",
    "                    f\"{CRM_URL}/annotations?\"\n",
    "                    f\"$filter=_objectid_value eq {app['new_jobapplicationid']}&\"\n",
    "                    \"$select=filename,mimetype,documentbody\"\n",
    "                )\n",
    "                try:\n",
    "                    annotations_response = session.get(annotations_url)\n",
    "                    annotations_response.raise_for_status()\n",
    "                    annotations_data = annotations_response.json()\n",
    "                    if \"value\" in annotations_data:\n",
    "                        app[\"annotations\"] = annotations_data[\"value\"]\n",
    "                    else:\n",
    "                        app[\"annotations\"] = []\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    print(f\"[CRM][ERROR] Error fetching annotations for application {app['new_jobapplicationid']}: {str(e)}\")\n",
    "                    app[\"annotations\"] = []\n",
    "            all_applications.extend(applications)\n",
    "            print(f\"[CRM] Total applications fetched: {len(all_applications)}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"[CRM][ERROR] Error fetching applications: {str(e)}\")\n",
    "    return all_applications\n",
    "\n",
    "def download_attachments(applications):\n",
    "    if not os.path.exists(DOWNLOAD_DIR):\n",
    "        os.makedirs(DOWNLOAD_DIR)\n",
    "    downloaded = 0\n",
    "    skipped = 0\n",
    "    for app in applications:\n",
    "        try:\n",
    "            app_id = app.get(\"new_jobapplicationid\")\n",
    "            created_date = app.get(\"createdon\", \"\").split(\"T\")[0]\n",
    "            annotations = app.get(\"annotations\", [])\n",
    "            for annotation in annotations:\n",
    "                if \"documentbody\" in annotation and \"filename\" in annotation:\n",
    "                    filename = annotation[\"filename\"]\n",
    "                    if filename.lower().endswith((\".pdf\", \".doc\", \".docx\")):\n",
    "                        try:\n",
    "                            unique_filename = f\"{created_date}_{filename}\"\n",
    "                            file_path = os.path.join(DOWNLOAD_DIR, unique_filename)\n",
    "                            if os.path.exists(file_path):\n",
    "                                print(f\"[CRM] Skipping existing file: {unique_filename}\")\n",
    "                                skipped += 1\n",
    "                                continue\n",
    "                            file_content = base64.b64decode(annotation[\"documentbody\"])\n",
    "                            with open(file_path, \"wb\") as f:\n",
    "                                f.write(file_content)\n",
    "                            print(f\"[CRM] Downloaded: {unique_filename}\")\n",
    "                            downloaded += 1\n",
    "                        except Exception as e:\n",
    "                            print(f\"[CRM][ERROR] Error saving file {filename}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[CRM][ERROR] Error processing application {app.get('new_jobapplicationid')}: {e}\")\n",
    "    print(f\"[CRM] Downloaded {downloaded} new files, skipped {skipped} existing files.\")\n",
    "    return downloaded, skipped\n",
    "\n",
    "def update_db_with_extra_columns(applications):\n",
    "    conn = psycopg2.connect(**DB_CONFIG)\n",
    "    cursor = conn.cursor()\n",
    "    updated = 0\n",
    "    try:\n",
    "        for app in applications:\n",
    "            app_id = app.get(\"new_jobapplicationid\")\n",
    "            name = app.get(\"new_name\")\n",
    "            email = app.get(\"new_email\")\n",
    "            jauid = app.get(\"new_jauid\")\n",
    "            if not app_id:\n",
    "                continue\n",
    "            cursor.execute(\"\"\"\n",
    "                UPDATE pdf_extracted_data SET\n",
    "                    crm_name2 = %s,\n",
    "                    crm_email2 = %s,\n",
    "                    crm_jauid2 = %s\n",
    "                WHERE crm_applicationid = %s\n",
    "            \"\"\", (name, email, jauid, app_id))\n",
    "            if cursor.rowcount > 0:\n",
    "                updated += cursor.rowcount\n",
    "        conn.commit()\n",
    "        print(f\"[CRM] Updated {updated} rows with extra CRM columns.\")\n",
    "    except Exception as e:\n",
    "        print(f\"[CRM][ERROR] Error updating database: {e}\")\n",
    "        conn.rollback()\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "print(\"[CRM] Fetching and downloading from CRM complete.\\n\")\n",
    "\n",
    "# Step 2: Extract OCR text from PDFs and store in database\n",
    "print(\"[OCR] Starting OCR extraction for new PDFs...\")\n",
    "def setup_database():\n",
    "    try:\n",
    "        conn = psycopg2.connect(**DB_CONFIG)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS pdf_extracted_data (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                pdf_filename VARCHAR(255),\n",
    "                ocr_result TEXT,\n",
    "                name VARCHAR(255),\n",
    "                email VARCHAR(255),\n",
    "                phone VARCHAR(255),\n",
    "                linkedin VARCHAR(255),\n",
    "                graduation_year VARCHAR(255),\n",
    "                university VARCHAR(255),\n",
    "                skills TEXT,\n",
    "                department VARCHAR(255),\n",
    "                job_title VARCHAR(255),\n",
    "                years_of_experience VARCHAR(50),\n",
    "                current_company VARCHAR(255),\n",
    "                location VARCHAR(255),\n",
    "                languages TEXT,\n",
    "                certifications TEXT,\n",
    "                project_types TEXT,\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "            )\n",
    "        \"\"\")\n",
    "        conn.commit()\n",
    "        return conn, cursor\n",
    "    except Exception as e:\n",
    "        print(f\"[OCR][ERROR] Database connection error: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        images = convert_from_path(\n",
    "            pdf_path,\n",
    "            poppler_path=POPPLER_PATH\n",
    "        )\n",
    "        text = \"\"\n",
    "        for img in images:\n",
    "            try:\n",
    "                text += pytesseract.image_to_string(img) + \"\\n\"\n",
    "            except Exception as e:\n",
    "                print(f\"[OCR][ERROR] OCR error on page: {e}\")\n",
    "                continue\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"[OCR][ERROR] Error processing PDF {pdf_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_pdfs_in_directory(directory_path):\n",
    "    conn, cursor = setup_database()\n",
    "    if not conn or not cursor:\n",
    "        print(\"[OCR][ERROR] Failed to setup database connection. Exiting.\")\n",
    "        return\n",
    "    os.makedirs(directory_path, exist_ok=True)\n",
    "    conn.autocommit = False\n",
    "    try:\n",
    "        cursor.execute(\"SELECT pdf_filename FROM pdf_extracted_data WHERE ocr_result IS NOT NULL\")\n",
    "        processed_files = {row[0] for row in cursor.fetchall()}\n",
    "        print(f\"[OCR] Found {len(processed_files)} already processed files in database\")\n",
    "        new_files = 0\n",
    "        failed_files = 0\n",
    "        for filename in os.listdir(directory_path):\n",
    "            if filename.endswith(\".pdf\"):\n",
    "                if filename in processed_files:\n",
    "                    print(f\"[OCR] Skipping already processed file: {filename}\")\n",
    "                    continue\n",
    "                file_path = os.path.join(directory_path, filename)\n",
    "                print(f\"[OCR] Processing new file: {filename}\")\n",
    "                new_files += 1\n",
    "                ocr_text = extract_text_from_pdf(file_path)\n",
    "                if not ocr_text:\n",
    "                    print(f\"[OCR][ERROR] Failed to extract text from {filename}\")\n",
    "                    failed_files += 1\n",
    "                    continue\n",
    "                try:\n",
    "                    cursor.execute(\"SELECT id FROM pdf_extracted_data WHERE pdf_filename = %s\", (filename,))\n",
    "                    if cursor.fetchone():\n",
    "                        print(f\"[OCR] File was processed by another process while we were working: {filename}\")\n",
    "                        continue\n",
    "                    cursor.execute(\n",
    "                        \"\"\"\n",
    "                        INSERT INTO pdf_extracted_data \n",
    "                        (pdf_filename, ocr_result)\n",
    "                        VALUES (%s, %s)\n",
    "                        \"\"\",\n",
    "                        (filename, ocr_text)\n",
    "                    )\n",
    "                    conn.commit()\n",
    "                    print(f\"[OCR] ✓ Successfully saved OCR text for: {filename}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"[OCR][ERROR] Database insertion error for {filename}: {e}\")\n",
    "                    conn.rollback()\n",
    "                    failed_files += 1\n",
    "                    continue\n",
    "        print(f\"[OCR] OCR Processing completed: {new_files - failed_files} files processed, {failed_files} failed, {new_files} total attempted.\")\n",
    "    except Exception as e:\n",
    "        print(f\"[OCR][ERROR] Error during processing: {e}\")\n",
    "        conn.rollback()\n",
    "    finally:\n",
    "        conn.autocommit = True\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "print(\"[OCR] OCR extraction complete.\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CRM] Starting CRM fetch and download process...\n",
      "[CRM] Fetching and downloading from CRM complete.\n",
      "\n",
      "[OCR] Starting OCR extraction for new PDFs...\n",
      "[OCR] OCR extraction complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Download CVs from CRM and update database (last 3 days, with detailed logging)\n",
    "print(\"[CRM] Starting CRM fetch and download process...\")\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_session():\n",
    "    session = requests.Session()\n",
    "    session.auth = HttpNtlmAuth(USERNAME, PASSWORD)\n",
    "    session.headers.update({\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"OData-MaxVersion\": \"4.0\",\n",
    "        \"OData-Version\": \"4.0\"\n",
    "    })\n",
    "    return session\n",
    "\n",
    "# Always fetch the last 3 days (not using MAX(createdon))\n",
    "def get_job_applications(session):\n",
    "    all_applications = []\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=3)\n",
    "    print(f\"[CRM] Fetching applications from {start_date} to {end_date.date()}\")\n",
    "    filter_condition = (\n",
    "        f\"createdon ge {start_date.strftime('%Y-%m-%dT00:00:00Z')} \"\n",
    "        f\"and createdon le {end_date.strftime('%Y-%m-%dT23:59:59Z')}\"\n",
    "    )\n",
    "    url = (\n",
    "        f\"{CRM_URL}/new_jobapplications?\"\n",
    "        f\"$select=new_jobapplicationid,new_name,new_email,new_jauid,createdon&\"\n",
    "        f\"$filter={quote(filter_condition)}&\"\n",
    "        \"$top=5000\"\n",
    "    )\n",
    "    try:\n",
    "        response = session.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        if \"value\" in data:\n",
    "            applications = data[\"value\"]\n",
    "            print(f\"[CRM] Found {len(applications)} applications in the last 3 days\")\n",
    "            for app in applications:\n",
    "                print(f\"  [CRM] AppID: {app.get('new_jobapplicationid')} | Name: {app.get('new_name')} | Created: {app.get('createdon')}\")\n",
    "                annotations_url = (\n",
    "                    f\"{CRM_URL}/annotations?\"\n",
    "                    f\"$filter=_objectid_value eq {app['new_jobapplicationid']}&\"\n",
    "                    \"$select=filename,mimetype,documentbody\"\n",
    "                )\n",
    "                try:\n",
    "                    annotations_response = session.get(annotations_url)\n",
    "                    annotations_response.raise_for_status()\n",
    "                    annotations_data = annotations_response.json()\n",
    "                    if \"value\" in annotations_data:\n",
    "                        app[\"annotations\"] = annotations_data[\"value\"]\n",
    "                        for annotation in annotations_data[\"value\"]:\n",
    "                            print(f\"    [CRM] Annotation filename: {annotation.get('filename')}\")\n",
    "                    else:\n",
    "                        app[\"annotations\"] = []\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    print(f\"[CRM][ERROR] Error fetching annotations for application {app['new_jobapplicationid']}: {str(e)}\")\n",
    "                    app[\"annotations\"] = []\n",
    "            all_applications.extend(applications)\n",
    "            print(f\"[CRM] Total applications fetched: {len(all_applications)}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"[CRM][ERROR] Error fetching applications: {str(e)}\")\n",
    "    return all_applications\n",
    "\n",
    "def get_last_extracted_date():\n",
    "    conn = psycopg2.connect(**DB_CONFIG)\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        cursor.execute(\"SELECT MAX(createdon) FROM pdf_extracted_data\")\n",
    "        last_date = cursor.fetchone()[0]\n",
    "        if last_date:\n",
    "            print(f\"[CRM] Last extracted date from DB: {last_date}\")\n",
    "            return last_date\n",
    "        else:\n",
    "            print(\"[CRM] No previous extraction found, defaulting to 3 days ago.\")\n",
    "            return datetime.now() - timedelta(days=3)\n",
    "    except Exception as e:\n",
    "        print(f\"[CRM][ERROR] Error fetching last extracted date: {e}\")\n",
    "        return datetime.now() - timedelta(days=3)\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "def download_attachments(applications):\n",
    "    if not os.path.exists(DOWNLOAD_DIR):\n",
    "        os.makedirs(DOWNLOAD_DIR)\n",
    "    downloaded = 0\n",
    "    skipped = 0\n",
    "    for app in applications:\n",
    "        try:\n",
    "            app_id = app.get(\"new_jobapplicationid\")\n",
    "            created_date = app.get(\"createdon\", \"\").split(\"T\")[0]\n",
    "            annotations = app.get(\"annotations\", [])\n",
    "            for annotation in annotations:\n",
    "                if \"documentbody\" in annotation and \"filename\" in annotation:\n",
    "                    filename = annotation[\"filename\"]\n",
    "                    if filename.lower().endswith((\".pdf\", \".doc\", \".docx\")):\n",
    "                        try:\n",
    "                            unique_filename = f\"{created_date}_{filename}\"\n",
    "                            file_path = os.path.join(DOWNLOAD_DIR, unique_filename)\n",
    "                            if os.path.exists(file_path):\n",
    "                                print(f\"[CRM] Skipping existing file: {unique_filename}\")\n",
    "                                skipped += 1\n",
    "                                continue\n",
    "                            file_content = base64.b64decode(annotation[\"documentbody\"])\n",
    "                            with open(file_path, \"wb\") as f:\n",
    "                                f.write(file_content)\n",
    "                            print(f\"[CRM] Downloaded: {unique_filename}\")\n",
    "                            downloaded += 1\n",
    "                        except Exception as e:\n",
    "                            print(f\"[CRM][ERROR] Error saving file {filename}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[CRM][ERROR] Error processing application {app.get('new_jobapplicationid')}: {e}\")\n",
    "    print(f\"[CRM] Downloaded {downloaded} new files, skipped {skipped} existing files.\")\n",
    "    return downloaded, skipped\n",
    "\n",
    "def update_db_with_extra_columns(applications):\n",
    "    conn = psycopg2.connect(**DB_CONFIG)\n",
    "    cursor = conn.cursor()\n",
    "    updated = 0\n",
    "    try:\n",
    "        for app in applications:\n",
    "            app_id = app.get(\"new_jobapplicationid\")\n",
    "            name = app.get(\"new_name\")\n",
    "            email = app.get(\"new_email\")\n",
    "            jauid = app.get(\"new_jauid\")\n",
    "            if not app_id:\n",
    "                continue\n",
    "            cursor.execute(\"\"\"\n",
    "                UPDATE pdf_extracted_data SET\n",
    "                    crm_name2 = %s,\n",
    "                    crm_email2 = %s,\n",
    "                    crm_jauid2 = %s\n",
    "                WHERE crm_applicationid = %s\n",
    "            \"\"\", (name, email, jauid, app_id))\n",
    "            if cursor.rowcount > 0:\n",
    "                updated += cursor.rowcount\n",
    "        conn.commit()\n",
    "        print(f\"[CRM] Updated {updated} rows with extra CRM columns.\")\n",
    "    except Exception as e:\n",
    "        print(f\"[CRM][ERROR] Error updating database: {e}\")\n",
    "        conn.rollback()\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "print(\"[CRM] Fetching and downloading from CRM complete.\\n\")\n",
    "\n",
    "# Step 2: Extract OCR text from PDFs and store in database\n",
    "print(\"[OCR] Starting OCR extraction for new PDFs...\")\n",
    "def setup_database():\n",
    "    try:\n",
    "        conn = psycopg2.connect(**DB_CONFIG)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS pdf_extracted_data (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                pdf_filename VARCHAR(255),\n",
    "                ocr_result TEXT,\n",
    "                name VARCHAR(255),\n",
    "                email VARCHAR(255),\n",
    "                phone VARCHAR(255),\n",
    "                linkedin VARCHAR(255),\n",
    "                graduation_year VARCHAR(255),\n",
    "                university VARCHAR(255),\n",
    "                skills TEXT,\n",
    "                department VARCHAR(255),\n",
    "                job_title VARCHAR(255),\n",
    "                years_of_experience VARCHAR(50),\n",
    "                current_company VARCHAR(255),\n",
    "                location VARCHAR(255),\n",
    "                languages TEXT,\n",
    "                certifications TEXT,\n",
    "                project_types TEXT,\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "            )\n",
    "        \"\"\")\n",
    "        conn.commit()\n",
    "        return conn, cursor\n",
    "    except Exception as e:\n",
    "        print(f\"[OCR][ERROR] Database connection error: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        images = convert_from_path(\n",
    "            pdf_path,\n",
    "            poppler_path=POPPLER_PATH\n",
    "        )\n",
    "        text = \"\"\n",
    "        for img in images:\n",
    "            try:\n",
    "                text += pytesseract.image_to_string(img) + \"\\n\"\n",
    "            except Exception as e:\n",
    "                print(f\"[OCR][ERROR] OCR error on page: {e}\")\n",
    "                continue\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"[OCR][ERROR] Error processing PDF {pdf_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_pdfs_in_directory(directory_path):\n",
    "    conn, cursor = setup_database()\n",
    "    if not conn or not cursor:\n",
    "        print(\"[OCR][ERROR] Failed to setup database connection. Exiting.\")\n",
    "        return\n",
    "    os.makedirs(directory_path, exist_ok=True)\n",
    "    conn.autocommit = False\n",
    "    try:\n",
    "        cursor.execute(\"SELECT pdf_filename FROM pdf_extracted_data WHERE ocr_result IS NOT NULL\")\n",
    "        processed_files = {row[0] for row in cursor.fetchall()}\n",
    "        print(f\"[OCR] Found {len(processed_files)} already processed files in database\")\n",
    "        new_files = 0\n",
    "        failed_files = 0\n",
    "        for filename in os.listdir(directory_path):\n",
    "            if filename.endswith(\".pdf\"):\n",
    "                if filename in processed_files:\n",
    "                    print(f\"[OCR] Skipping already processed file: {filename}\")\n",
    "                    continue\n",
    "                file_path = os.path.join(directory_path, filename)\n",
    "                print(f\"[OCR] Processing new file: {filename}\")\n",
    "                new_files += 1\n",
    "                ocr_text = extract_text_from_pdf(file_path)\n",
    "                if not ocr_text:\n",
    "                    print(f\"[OCR][ERROR] Failed to extract text from {filename}\")\n",
    "                    failed_files += 1\n",
    "                    continue\n",
    "                try:\n",
    "                    cursor.execute(\"SELECT id FROM pdf_extracted_data WHERE pdf_filename = %s\", (filename,))\n",
    "                    if cursor.fetchone():\n",
    "                        print(f\"[OCR] File was processed by another process while we were working: {filename}\")\n",
    "                        continue\n",
    "                    cursor.execute(\n",
    "                        \"\"\"\n",
    "                        INSERT INTO pdf_extracted_data \n",
    "                        (pdf_filename, ocr_result)\n",
    "                        VALUES (%s, %s)\n",
    "                        \"\"\",\n",
    "                        (filename, ocr_text)\n",
    "                    )\n",
    "                    conn.commit()\n",
    "                    print(f\"[OCR] ✓ Successfully saved OCR text for: {filename}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"[OCR][ERROR] Database insertion error for {filename}: {e}\")\n",
    "                    conn.rollback()\n",
    "                    failed_files += 1\n",
    "                    continue\n",
    "        print(f\"[OCR] OCR Processing completed: {new_files - failed_files} files processed, {failed_files} failed, {new_files} total attempted.\")\n",
    "    except Exception as e:\n",
    "        print(f\"[OCR][ERROR] Error during processing: {e}\")\n",
    "        conn.rollback()\n",
    "    finally:\n",
    "        conn.autocommit = True\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "print(\"[OCR] OCR extraction complete.\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPT] Starting GPT extraction for unprocessed records...\n",
      "[GPT] GPT extraction complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Use GPT to extract structured info from OCR text\n",
    "print(\"[GPT] Starting GPT extraction for unprocessed records...\")\n",
    "def extract_info_with_gpt(ocr_text):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"Extract the following information from the CV text in JSON format:\\n                    - name: Full name of the candidate\\n                    - email: Email address\\n                    - phone: Phone number\\n                    - linkedin: LinkedIn profile URL if present\\n                    - graduation_year: Year of graduation\\n                    - university: University name\\n                    - skills: List of technical and soft skills\\n                    - department: Normalize to one of: [Engineering, IT/Software Development, Sales, Marketing, HR, Finance/Accounting, Operations, Legal, Administrative, Other]\\n                    - job_title: Normalize to closest match of: [Software Engineer, Project Manager, Business Analyst, Sales Representative, Marketing Specialist, HR Manager, Financial Analyst, Operations Manager, Legal Counsel, Administrative Assistant]\\n                    - years_of_experience: Total years of experience\\n                    - current_company: Current or most recent company\\n                    - location: City/Country\\n                    - languages: List of languages known\\n                    - certifications: List of certifications\\n                    - project_types: Types of projects worked on\"\"\"\n",
    "            }, {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": ocr_text[:4000]\n",
    "            }],\n",
    "            temperature=0.1,\n",
    "            max_tokens=1000\n",
    "        )\n",
    "        result = json.loads(response.choices[0].message.content)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"[GPT][ERROR] GPT extraction error: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_unprocessed_records():\n",
    "    conn, cursor = setup_database()\n",
    "    if not conn or not cursor:\n",
    "        print(\"[GPT][ERROR] Could not connect to database.\")\n",
    "        return\n",
    "    try:\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT id, pdf_filename, ocr_result \n",
    "            FROM pdf_extracted_data \n",
    "            WHERE ocr_result IS NOT NULL \n",
    "            AND (name IS NULL OR department IS NULL OR job_title IS NULL)\n",
    "        \"\"\")\n",
    "        records = cursor.fetchall()\n",
    "        print(f\"[GPT] Found {len(records)} records to process with GPT.\")\n",
    "        processed = 0\n",
    "        failed = 0\n",
    "        for record_id, filename, ocr_text in records:\n",
    "            print(f\"[GPT] Processing: {filename}\")\n",
    "            info = extract_info_with_gpt(ocr_text)\n",
    "            if not info:\n",
    "                print(f\"[GPT][ERROR] Failed to extract information for {filename}\")\n",
    "                failed += 1\n",
    "                continue\n",
    "            try:\n",
    "                cursor.execute(\"\"\"\n",
    "                    UPDATE pdf_extracted_data \n",
    "                    SET name = %s,\n",
    "                        email = %s,\n",
    "                        phone = %s,\n",
    "                        linkedin = %s,\n",
    "                        graduation_year = %s,\n",
    "                        university = %s,\n",
    "                        skills = %s,\n",
    "                        department = %s,\n",
    "                        job_title = %s,\n",
    "                        years_of_experience = %s,\n",
    "                        current_company = %s,\n",
    "                        location = %s,\n",
    "                        languages = %s,\n",
    "                        certifications = %s,\n",
    "                        project_types = %s\n",
    "                    WHERE id = %s\n",
    "                \"\"\", (\n",
    "                    info.get('name', ''),\n",
    "                    info.get('email', ''),\n",
    "                    info.get('phone', ''),\n",
    "                    info.get('linkedin', ''),\n",
    "                    info.get('graduation_year', ''),\n",
    "                    info.get('university', ''),\n",
    "                    json.dumps(info.get('skills', [])),\n",
    "                    info.get('department', ''),\n",
    "                    info.get('job_title', ''),\n",
    "                    info.get('years_of_experience', ''),\n",
    "                    info.get('current_company', ''),\n",
    "                    info.get('location', ''),\n",
    "                    json.dumps(info.get('languages', [])),\n",
    "                    json.dumps(info.get('certifications', [])),\n",
    "                    json.dumps(info.get('project_types', [])),\n",
    "                    record_id\n",
    "                ))\n",
    "                conn.commit()\n",
    "                processed += 1\n",
    "                print(f\"[GPT] ✓ Successfully processed {filename}\")\n",
    "                print(f\"      Department: {info.get('department', 'N/A')}\")\n",
    "                print(f\"      Job Title: {info.get('job_title', 'N/A')}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[GPT][ERROR] Database update error for {filename}: {e}\")\n",
    "                conn.rollback()\n",
    "                failed += 1\n",
    "        print(f\"[GPT] Processing completed.\")\n",
    "        print(f\"[GPT] ✓ Successfully processed: {processed} records\")\n",
    "        print(f\"[GPT] ❌ Failed to process: {failed} records\")\n",
    "        print(f\"[GPT] Total cost estimate: ${(processed + failed) * 0.003:.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[GPT][ERROR] Error during processing: {e}\")\n",
    "        conn.rollback()\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "print(\"[GPT] GPT extraction complete.\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CRM] Adding extra CRM columns if missing...\n",
      "[CRM] Ensured all extra CRM columns exist in the database.\n",
      "[CRM] Fetching CRM data and annotation filenames...\n",
      "[CRM] Fetching job applications from 2025-05-30 to 2025-06-02 with fields: new_jobapplicationid, new_fullname, new_contactphone, new_telephonenumber, new_jauid, new_jobofferstatus, new_gender, new_position, new_employmenttype, new_expectedsalary, new_dateavailableforemployment, new_currentsalary, new_company, new_graduationyear, new_qualitiesattributes, new_careergoals, new_additionalinformation, new_appstatus, new_hrinterviewstatus, new_technicalrating, new_technicalinterviewcomments, new_hrcomment, createdon, modifiedon, new_howdidyouhearaboutrowad, new_listouttheextrasocialactivities, new_pleasesepcify\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Omar Essam2\\AppData\\Local\\Temp\\2\\ipykernel_16240\\2924060929.py:86: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  today = datetime.utcnow().date()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CRM] Fetched 37 CRM records with annotation filenames.\n",
      "[CRM] Updating database with CRM fields (only where NULL)...\n",
      "[CRM] Updated 0 rows in pdf_extracted_data with extra CRM fields (only where NULL).\n",
      "[CRM] Done.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Add extra CRM fields to the database (last 3 days only, by normalized filename, only fill NULL fields)\n",
    "import os\n",
    "import requests\n",
    "from requests_ntlm import HttpNtlmAuth\n",
    "from dotenv import load_dotenv\n",
    "import psycopg2\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "CRM_URL = \"https://rmecrm.rowad-rme.com/RMECRM/api/data/v8.2\"\n",
    "USERNAME = \"Rowad\\\\Omar Essam\"\n",
    "PASSWORD = \"PMO@1234\"\n",
    "\n",
    "DB_CONFIG = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"database\": \"postgres\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"PMO@1234\"\n",
    "}\n",
    "\n",
    "# List of CRM fields to extract and update\n",
    "CRM_FIELDS = [\n",
    "    \"new_jobapplicationid\",\n",
    "    \"new_fullname\",\n",
    "    \"new_contactphone\",\n",
    "    \"new_telephonenumber\",\n",
    "    \"new_jauid\",\n",
    "    \"new_jobofferstatus\",\n",
    "    \"new_gender\",\n",
    "    \"new_position\",\n",
    "    \"new_employmenttype\",\n",
    "    \"new_expectedsalary\",\n",
    "    \"new_dateavailableforemployment\",\n",
    "    \"new_currentsalary\",\n",
    "    \"new_company\",\n",
    "    \"new_graduationyear\",\n",
    "    \"new_qualitiesattributes\",\n",
    "    \"new_careergoals\",\n",
    "    \"new_additionalinformation\",\n",
    "    \"new_appstatus\",\n",
    "    \"new_hrinterviewstatus\",\n",
    "    \"new_technicalrating\",\n",
    "    \"new_technicalinterviewcomments\",\n",
    "    \"new_hrcomment\",\n",
    "    \"createdon\",\n",
    "    \"modifiedon\",\n",
    "    \"new_howdidyouhearaboutrowad\",\n",
    "    \"new_listouttheextrasocialactivities\",\n",
    "    \"new_pleasesepcify\"\n",
    "]\n",
    "\n",
    "# Map CRM fields to DB columns (prefix with crm_)\n",
    "CRM_TO_DB = {field: f\"crm_{field[4:]}\" if field.startswith(\"new_\") else f\"crm_{field}\" for field in CRM_FIELDS}\n",
    "\n",
    "# Add columns to DB if missing\n",
    "def add_crm_columns():\n",
    "    with psycopg2.connect(**DB_CONFIG) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            for crm_field, db_col in CRM_TO_DB.items():\n",
    "                cur.execute(f\"\"\"\n",
    "                    DO $$\n",
    "                    BEGIN\n",
    "                        IF NOT EXISTS (\n",
    "                            SELECT 1 FROM information_schema.columns \n",
    "                            WHERE table_name = 'pdf_extracted_data' \n",
    "                            AND column_name = '{db_col}'\n",
    "                        ) THEN\n",
    "                            ALTER TABLE pdf_extracted_data ADD COLUMN {db_col} TEXT;\n",
    "                        END IF;\n",
    "                    END $$;\n",
    "                \"\"\")\n",
    "            conn.commit()\n",
    "    print(\"[CRM] Ensured all extra CRM columns exist in the database.\")\n",
    "\n",
    "# Fetch CRM data and annotation filenames (last 3 days only)\n",
    "def fetch_crm_data():\n",
    "    session = requests.Session()\n",
    "    session.auth = HttpNtlmAuth(USERNAME, PASSWORD)\n",
    "    session.headers.update({\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"OData-MaxVersion\": \"4.0\",\n",
    "        \"OData-Version\": \"4.0\"\n",
    "    })\n",
    "    today = datetime.utcnow().date()\n",
    "    days_ago = today - timedelta(days=3)\n",
    "    filter_condition = f\"createdon ge {days_ago}T00:00:00Z and createdon le {today}T23:59:59Z\"\n",
    "    url = f\"{CRM_URL}/new_jobapplications?$select={','.join(CRM_FIELDS)}&$filter={filter_condition}&$top=5000\"\n",
    "    print(f\"[CRM] Fetching job applications from {days_ago} to {today} with fields: {', '.join(CRM_FIELDS)}\")\n",
    "    response = session.get(url)\n",
    "    response.raise_for_status()\n",
    "    applications = response.json().get(\"value\", [])\n",
    "    crm_data = []\n",
    "    for app in applications:\n",
    "        app_id = app.get(\"new_jobapplicationid\")\n",
    "        createdon = app.get(\"createdon\")\n",
    "        if not (app_id and createdon):\n",
    "            continue\n",
    "        created_date = str(createdon).split('T')[0]\n",
    "        # Fetch annotations for this application\n",
    "        annotations_url = f\"{CRM_URL}/annotations?$filter=_objectid_value eq {app_id}&$select=filename\"\n",
    "        ann_response = session.get(annotations_url)\n",
    "        ann_response.raise_for_status()\n",
    "        annotations = ann_response.json().get('value', [])\n",
    "        for annotation in annotations:\n",
    "            filename = annotation.get('filename')\n",
    "            if filename:\n",
    "                constructed_filename = f\"{created_date}_{filename}\"\n",
    "                normalized = constructed_filename.replace(' ', '').lower()\n",
    "                crm_data.append({\n",
    "                    'normalized_filename': normalized,\n",
    "                    **app\n",
    "                })\n",
    "    print(f\"[CRM] Fetched {len(crm_data)} CRM records with annotation filenames.\")\n",
    "    return crm_data\n",
    "\n",
    "# Update DB with CRM fields by normalized filename, only if currently NULL\n",
    "def update_db_with_crm_fields(crm_data):\n",
    "    with psycopg2.connect(**DB_CONFIG) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            updated = 0\n",
    "            for item in crm_data:\n",
    "                normalized_filename = item['normalized_filename']\n",
    "                set_clauses = []\n",
    "                values = []\n",
    "                null_conditions = []\n",
    "                for crm_field, db_col in CRM_TO_DB.items():\n",
    "                    value = item.get(crm_field)\n",
    "                    set_clauses.append(f\"{db_col} = %s\")\n",
    "                    values.append(value)\n",
    "                    null_conditions.append(f\"{db_col} IS NULL\")\n",
    "                set_clause = \", \".join(set_clauses)\n",
    "                null_condition = \" OR \".join(null_conditions)\n",
    "                values.append(normalized_filename)\n",
    "                cur.execute(f\"\"\"\n",
    "                    UPDATE pdf_extracted_data SET {set_clause}\n",
    "                    WHERE ( {null_condition} )\n",
    "                    AND LOWER(REPLACE(pdf_filename, ' ', '')) = %s\n",
    "                \"\"\", values)\n",
    "                if cur.rowcount > 0:\n",
    "                    updated += cur.rowcount\n",
    "            conn.commit()\n",
    "    print(f\"[CRM] Updated {updated} rows in pdf_extracted_data with extra CRM fields (only where NULL).\")\n",
    "\n",
    "# Run the process\n",
    "print(\"[CRM] Adding extra CRM columns if missing...\")\n",
    "add_crm_columns()\n",
    "print(\"[CRM] Fetching CRM data and annotation filenames...\")\n",
    "crm_data = fetch_crm_data()\n",
    "print(\"[CRM] Updating database with CRM fields (only where NULL)...\")\n",
    "update_db_with_crm_fields(crm_data)\n",
    "print(\"[CRM] Done.\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
