{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pdfplumber\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from docx import Document\n",
    "from pptx import Presentation\n",
    "import warnings\n",
    "import re\n",
    "import zipfile\n",
    "import rarfile\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "# Suppress specific CropBox warning from pdfplumber\n",
    "import logging\n",
    "logging.getLogger(\"pdfminer\").setLevel(logging.ERROR)\n",
    "\n",
    "class CropBoxFilter:\n",
    "    def filter(self, record):\n",
    "        return not (record.levelno == logging.WARNING and 'CropBox missing from /Page, defaulting to MediaBox' in record.getMessage())\n",
    "\n",
    "logging.getLogger().addFilter(CropBoxFilter())\n",
    "\n",
    "# Add tkinter for folder selection\n",
    "def select_folder():\n",
    "    import tkinter as tk\n",
    "    from tkinter import filedialog\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    folder_selected = filedialog.askdirectory(title='Select folder to process')\n",
    "    root.destroy()\n",
    "    return folder_selected\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text + \"\\n\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    return text.strip()\n",
    "\n",
    "def ocr_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                img = page.to_image(resolution=300).original\n",
    "                pil_img = Image.fromarray(img)\n",
    "                page_text = pytesseract.image_to_string(pil_img)\n",
    "                if page_text:\n",
    "                    text += page_text + \"\\n\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    return text.strip()\n",
    "\n",
    "def extract_project_name(file_path):\n",
    "    # Try to extract project name from folder path or filename\n",
    "    path_parts = os.path.normpath(file_path).split(os.sep)\n",
    "    # Remove common non-project folders\n",
    "    ignore = set(['', '2024', '2025', '2023', '2022', '2021', '2020', 'Mar. 2024', 'Jan-25', '3-Mar-2025', '01 Jan-25'])\n",
    "    # Look for a likely project name in the path\n",
    "    for part in reversed(path_parts[:-1]):\n",
    "        if part not in ignore and not re.match(r'\\d{4}', part):\n",
    "            return part\n",
    "    # Try to extract from filename\n",
    "    fname = os.path.basename(file_path)\n",
    "    match = re.search(r'([A-Za-z0-9\\- ]+Project|[A-Za-z0-9\\- ]+Dashboard|[A-Za-z0-9\\- ]+Lot|[A-Za-z0-9\\- ]+Port|[A-Za-z0-9\\- ]+Sector)', fname)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    # If nothing found, label as GLOBAL\n",
    "    return 'GLOBAL'\n",
    "\n",
    "def extract_text_from_excel(excel_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        ext = excel_path.lower().split('.')[-1]\n",
    "        if ext == 'xlsb':\n",
    "            xls = pd.ExcelFile(excel_path, engine='pyxlsb')\n",
    "        else:\n",
    "            xls = pd.ExcelFile(excel_path)\n",
    "        for sheet_name in xls.sheet_names:\n",
    "            df = pd.read_excel(xls, sheet_name=sheet_name, dtype=str)\n",
    "            # Only extract if <1000 rows and <30 columns and has headers\n",
    "            if df.shape[0] < 1000 and df.shape[1] < 30 and all(df.columns.str.strip() != ''):\n",
    "                text += f\"\\n--- Sheet: {sheet_name} ---\\n\"\n",
    "                text += df.fillna('').to_csv(index=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return text.strip()\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        doc = Document(docx_path)\n",
    "        for para in doc.paragraphs:\n",
    "            text += para.text + \"\\n\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    return text.strip()\n",
    "\n",
    "def extract_text_from_pptx(pptx_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        prs = Presentation(pptx_path)\n",
    "        for slide in prs.slides:\n",
    "            for shape in slide.shapes:\n",
    "                if hasattr(shape, \"text\"):\n",
    "                    text += shape.text + \"\\n\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    return text.strip()\n",
    "\n",
    "def extract_text_from_ppt(file_path):\n",
    "    \"\"\"Attempt to convert .ppt to .pptx using unoconv, then extract text.\"\"\"\n",
    "    import subprocess\n",
    "    import tempfile\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            pptx_path = os.path.join(tmpdir, os.path.basename(file_path) + '.pptx')\n",
    "            # Try to convert .ppt to .pptx using unoconv\n",
    "            result = subprocess.run([\n",
    "                'unoconv', '-f', 'pptx', '-o', pptx_path, file_path\n",
    "            ], capture_output=True)\n",
    "            if result.returncode == 0 and os.path.exists(pptx_path):\n",
    "                text = extract_text_from_pptx(pptx_path)\n",
    "            else:\n",
    "                print(f\"[WARN] Failed to convert .ppt to .pptx: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Exception during .ppt conversion: {file_path} | {e}\")\n",
    "    return text.strip()\n",
    "\n",
    "def extract_text_from_pbix(pbix_path):\n",
    "    \"\"\"Try to extract text from .pbix by unzipping and reading .json/.xml files inside.\"\"\"\n",
    "    import zipfile\n",
    "    import tempfile\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with zipfile.ZipFile(pbix_path, 'r') as z:\n",
    "            with tempfile.TemporaryDirectory() as tmpdir:\n",
    "                z.extractall(tmpdir)\n",
    "                for root, _, files in os.walk(tmpdir):\n",
    "                    for file in files:\n",
    "                        if file.endswith('.json') or file.endswith('.xml'):\n",
    "                            try:\n",
    "                                with open(os.path.join(root, file), 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                                    content = f.read()\n",
    "                                    # Only keep text if it's not too large\n",
    "                                    if len(content) < 500_000:\n",
    "                                        text += f\"\\n--- {file} ---\\n\" + content\n",
    "                            except Exception:\n",
    "                                continue\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to extract .pbix: {pbix_path} | {e}\")\n",
    "    return text.strip()\n",
    "\n",
    "def extract_text_from_doc(doc_path):\n",
    "    \"\"\"Convert .doc to .docx using unoconv, then extract text.\"\"\"\n",
    "    import subprocess\n",
    "    import tempfile\n",
    "    import os\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            docx_path = os.path.join(tmpdir, os.path.basename(doc_path) + '.docx')\n",
    "            result = subprocess.run([\n",
    "                'unoconv', '-f', 'docx', '-o', docx_path, doc_path\n",
    "            ], capture_output=True)\n",
    "            if result.returncode == 0 and os.path.exists(docx_path):\n",
    "                text = extract_text_from_docx(docx_path)\n",
    "            else:\n",
    "                print(f\"[WARN] Failed to convert .doc to .docx: {doc_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Exception during .doc conversion: {doc_path} | {e}\")\n",
    "    return text.strip()\n",
    "\n",
    "def process_file(file_path, data):\n",
    "    ext = file_path.lower().split('.')[-1]\n",
    "    project = extract_project_name(file_path)\n",
    "    print(f\"[INFO] Processing file: {os.path.basename(file_path)} | Project: {project}\")\n",
    "    entry = {\n",
    "        'file_name': os.path.basename(file_path),\n",
    "        'file_path': file_path,\n",
    "        'type': ext,\n",
    "        'project': project,\n",
    "        'extracted_at': datetime.now().isoformat(),\n",
    "        'text': ''\n",
    "    }\n",
    "    reason = None\n",
    "    if ext == 'pdf':\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "        if not text:\n",
    "            text = ocr_pdf(file_path)\n",
    "            if text:\n",
    "                reason = 'extracted via OCR'\n",
    "            else:\n",
    "                reason = 'no text extracted (PDF and OCR failed)'\n",
    "        else:\n",
    "            reason = 'extracted as text PDF'\n",
    "        entry['text'] = text\n",
    "    elif ext in ['xls', 'xlsx', 'xlsb']:\n",
    "        text = extract_text_from_excel(file_path)\n",
    "        entry['text'] = text\n",
    "        reason = 'extracted from Excel' if text else 'no text extracted (Excel)'\n",
    "    elif ext == 'docx':\n",
    "        text = extract_text_from_docx(file_path)\n",
    "        entry['text'] = text\n",
    "        reason = 'extracted from Word' if text else 'no text extracted (Word)'\n",
    "    elif ext in ['pptx', 'pptm']:\n",
    "        text = extract_text_from_pptx(file_path)\n",
    "        entry['text'] = text\n",
    "        reason = 'extracted from PowerPoint' if text else 'no text extracted (PowerPoint)'\n",
    "    elif ext == 'ppt':\n",
    "        text = extract_text_from_ppt(file_path)\n",
    "        entry['text'] = text\n",
    "        reason = 'extracted from PPT (via conversion)' if text else 'no text extracted (PPT)'\n",
    "    elif ext == 'pbix':\n",
    "        text = extract_text_from_pbix(file_path)\n",
    "        entry['text'] = text\n",
    "        reason = 'extracted from PBIX' if text else 'no text extracted (PBIX)'\n",
    "    elif ext == 'doc':\n",
    "        text = extract_text_from_doc(file_path)\n",
    "        entry['text'] = text\n",
    "        reason = 'extracted from DOC (via conversion)' if text else 'no text extracted (DOC)'\n",
    "    else:\n",
    "        print(f\"[WARN] Skipped unsupported file type: {os.path.basename(file_path)}\")\n",
    "        return\n",
    "    if entry['text']:\n",
    "        data.append(entry)\n",
    "    else:\n",
    "        print(f\"[WARN] No text extracted from: {os.path.basename(file_path)} ({reason})\")\n",
    "\n",
    "def process_folder(folder_path, output_json_path):\n",
    "    data = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            ext = file.lower().split('.')[-1]\n",
    "            if ext in ['zip', 'rar']:\n",
    "                print(f\"[INFO] Extracting archive: {file}\")\n",
    "                with tempfile.TemporaryDirectory() as tmpdir:\n",
    "                    try:\n",
    "                        if ext == 'zip':\n",
    "                            with zipfile.ZipFile(file_path, 'r') as z:\n",
    "                                z.extractall(tmpdir)\n",
    "                        elif ext == 'rar':\n",
    "                            with rarfile.RarFile(file_path, 'r') as r:\n",
    "                                r.extractall(tmpdir)\n",
    "                        # Recursively process extracted files\n",
    "                        for subroot, _, subfiles in os.walk(tmpdir):\n",
    "                            for subfile in subfiles:\n",
    "                                subfile_path = os.path.join(subroot, subfile)\n",
    "                                process_file(subfile_path, data)\n",
    "                    except Exception as e:\n",
    "                        print(f\"[WARN] Failed to extract {file}: {e}\")\n",
    "            else:\n",
    "                process_file(file_path, data)\n",
    "    with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Processed {len(data)} documents. Output saved to {output_json_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # New output directory outside the repo\n",
    "    output_dir = r'C:/Users/Omar Essam2/OneDrive - Rowad Modern Engineering/x004 Data Science/03.rme.db/05.llm/extracted_json'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    folder = select_folder()\n",
    "    if not folder:\n",
    "        print(\"No folder selected. Exiting.\")\n",
    "    else:\n",
    "        folder_name = os.path.basename(os.path.normpath(folder))\n",
    "        output_json = os.path.join(output_dir, f\"{folder_name}_extracted.json\")\n",
    "        process_folder(folder, output_json) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirements: pip install sentence-transformers torch chromadb tqdm python-dotenv\n",
    "import os\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load environment variables (if needed for paths)\n",
    "load_dotenv()\n",
    "\n",
    "# New paths outside the repo\n",
    "EXTRACTED_DIR = r'C:/Users/Omar Essam2/OneDrive - Rowad Modern Engineering/x004 Data Science/03.rme.db/05.llm/extracted_json'\n",
    "CHROMA_DB_DIR = r'C:/Users/Omar Essam2/OneDrive - Rowad Modern Engineering/x004 Data Science/03.rme.db/05.llm/chroma_db_local'\n",
    "COLLECTION_NAME = 'company_docs_local'\n",
    "CHUNK_SIZE = 500  # characters per chunk (reduced for more precise retrieval)\n",
    "CHUNK_OVERLAP = 200\n",
    "ID_TRACK_FILE = os.path.join(CHROMA_DB_DIR, 'embedded_chunk_ids.txt')\n",
    "BATCH_SIZE = 32  # Number of chunks to embed in one call\n",
    "\n",
    "# Helper: chunk text\n",
    "def chunk_text(text, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "# Helper: get local embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # You can change to another local model if needed\n",
    "def get_embeddings(texts):\n",
    "    return model.encode(texts, show_progress_bar=False, convert_to_numpy=True).tolist()\n",
    "\n",
    "# Initialize Chroma DB\n",
    "client = chromadb.PersistentClient(path=CHROMA_DB_DIR, settings=Settings(allow_reset=True))\n",
    "if COLLECTION_NAME in [c.name for c in client.list_collections()]:\n",
    "    collection = client.get_collection(COLLECTION_NAME)\n",
    "else:\n",
    "    collection = client.create_collection(COLLECTION_NAME)\n",
    "\n",
    "def load_embedded_ids():\n",
    "    if not os.path.exists(ID_TRACK_FILE):\n",
    "        return set()\n",
    "    with open(ID_TRACK_FILE, 'r', encoding='utf-8') as f:\n",
    "        return set(line.strip() for line in f if line.strip())\n",
    "\n",
    "def save_embedded_id(chunk_id):\n",
    "    with open(ID_TRACK_FILE, 'a', encoding='utf-8') as f:\n",
    "        f.write(chunk_id + '\\n')\n",
    "\n",
    "def process_json_files():\n",
    "    files = [f for f in os.listdir(EXTRACTED_DIR) if f.endswith('.json')]\n",
    "    doc_count = 0\n",
    "    embedded_ids = load_embedded_ids()\n",
    "    for file in files:\n",
    "        with open(os.path.join(EXTRACTED_DIR, file), 'r', encoding='utf-8') as f:\n",
    "            docs = json.load(f)\n",
    "        for doc in tqdm(docs, desc=f\"Processing {file}\"):\n",
    "            text = doc['text']\n",
    "            if not text.strip():\n",
    "                continue\n",
    "            chunks = chunk_text(text)\n",
    "            batch_chunks = []\n",
    "            batch_metas = []\n",
    "            batch_ids = []\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunk_id = f\"{doc['file_name']}_{i}\"\n",
    "                if chunk_id in embedded_ids:\n",
    "                    continue  # Skip already embedded chunk\n",
    "                meta = {\n",
    "                    'file_name': doc['file_name'],\n",
    "                    'file_path': doc['file_path'],\n",
    "                    'type': doc['type'],\n",
    "                    'project': doc.get('project', 'GLOBAL'),\n",
    "                    'extracted_at': doc['extracted_at'],\n",
    "                    'chunk': i\n",
    "                }\n",
    "                batch_chunks.append(chunk)\n",
    "                batch_metas.append(meta)\n",
    "                batch_ids.append(chunk_id)\n",
    "                if len(batch_chunks) == BATCH_SIZE:\n",
    "                    try:\n",
    "                        embs = get_embeddings(batch_chunks)\n",
    "                        collection.add(\n",
    "                            documents=batch_chunks,\n",
    "                            embeddings=embs,\n",
    "                            metadatas=batch_metas,\n",
    "                            ids=batch_ids\n",
    "                        )\n",
    "                        for cid in batch_ids:\n",
    "                            save_embedded_id(cid)\n",
    "                        doc_count += len(batch_chunks)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error embedding batch: {e}\")\n",
    "                    batch_chunks, batch_metas, batch_ids = [], [], []\n",
    "            # Process any remaining chunks in the batch\n",
    "            if batch_chunks:\n",
    "                try:\n",
    "                    embs = get_embeddings(batch_chunks)\n",
    "                    collection.add(\n",
    "                        documents=batch_chunks,\n",
    "                        embeddings=embs,\n",
    "                        metadatas=batch_metas,\n",
    "                        ids=batch_ids\n",
    "                    )\n",
    "                    for cid in batch_ids:\n",
    "                        save_embedded_id(cid)\n",
    "                    doc_count += len(batch_chunks)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error embedding batch: {e}\")\n",
    "    print(f\"Finished! {doc_count} new chunks embedded and stored in Chroma DB.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_json_files() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
